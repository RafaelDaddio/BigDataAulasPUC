{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1f1ed2",
   "metadata": {},
   "source": [
    "\n",
    "# Introdução ao Spark\n",
    "\n",
    "Nesse Notebook iremos aprender algumas funcionalidades básicas de Spark. Aprenderemos Spark em Python através da biblioteca PySpark. Para instalá-la, basta fazer o download  e seguir os passos em: http://spark.apache.org/downloads.html, ou instalar usando Pypi:\n",
    "\n",
    "\tpip install pyspark\n",
    "\n",
    "Para utilizá-la em conjunto com um Jupyter Notebook, você precisa baixar a biblioteca Jupyter.\n",
    "\n",
    "\tpip install jupyter\n",
    "    \n",
    "Depois, baixar também a biblioteca findspark\n",
    "\n",
    "\tpip install findspark\n",
    "\n",
    "E pronto! Agora, no começo de cada notebook você deverá importar tanto `findspark` quanto `pyspark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316ec4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d97dfa",
   "metadata": {},
   "source": [
    "# O objeto `SparkContext`\n",
    "\n",
    "A primeira coisa a ser feita para rodar as funcionalidades do Spark é inicializar um contexto Spark. Isso é feito através de um objeto `SparkContext`. Ele é a principal entrada para as funcionalidades do Spark. É ele quem realiza e representa a conexão para um cluster, e é utilizado para criar RDDs e distribuir variáveis pelo cluster. \n",
    "\n",
    "![Retirado de: https://www.devmedia.com.br/introducao-ao-apache-spark/34178](https://arquivo.devmedia.com.br/artigos/Eduardo_Zambom/spark/image002.jpg)\n",
    "\n",
    "Quando utilizamos o Shell Spark, a própria API cria um objeto `SparkContext`, mas esse não é o nosso caso. Vamos criar um objeto `SparkContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2f1df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 14:29:42,903 WARN util.Utils: Your hostname, bigdatavm-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "2021-12-04 14:29:42,907 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-12-04 14:29:43,515 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName='Introducao Spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c727bb",
   "metadata": {},
   "source": [
    "Vamos ver o que contém no objeto `sc`. Observe que ele possui duas variáveis:\n",
    "    - **master:** onde está o cluster. Como estamos rodando localmente, o objeto seta essa variável como `local[*]`\n",
    "    - **appName:** o nome da aplicação. \n",
    "    \n",
    "Ambas variáveis são obrigatórias na criação de um contexto. Você poderia escolher apontar o master para um cluster Hadoop, fornecendo sua URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f37c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Introducao Spark>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af53900",
   "metadata": {},
   "source": [
    "# Criando RDDS\n",
    "\n",
    "Com o contexto inicializado, iremos começar nosso aprendizado com as APIs de baixo nível, os RDDs. RDDs são a unidade mais primitiva do Spark e recebem esta sigla de:\n",
    "- _Resilient_ \n",
    "- _Distributed_\n",
    "- _Datasets_\n",
    "\n",
    "\n",
    "![Retirado de: https://www.javatpoint.com/pyspark-rdd](https://static.javatpoint.com/tutorial/pyspark/images/pyspark-rdd2.png)\n",
    "\n",
    "Os RDDs são a unidade base em que todas as demais APIs de Spark são construídas. As APIs de mais alto nível, como Dataframes, SparkSQL, SparkStreaming, MLlib, etc. utilizam estas unidades como base para o processamento distribuído.\n",
    "\n",
    "![Retirado de: https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/assets/spdg_0101.png)\n",
    "\n",
    "Para criar RDDs, podemos fazer de dois modos:\n",
    "- Utilizando variáveis e coleções Python com `parallelize()`\n",
    "- Lendo e extraindo valores de arquivos textuais com `textFile()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6de8409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:274\n",
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# criando um RDD a partir de uma lista\n",
    "\n",
    "list_ex = [1,2,3,4,5,6]\n",
    "list_RDD = sc.parallelize(list_ex)\n",
    "print(list_RDD)\n",
    "print(list_RDD.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52dac2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Esse é um teste Spark!']\n"
     ]
    }
   ],
   "source": [
    "# criando um RDD a partir de um arquivo\n",
    "\n",
    "file = 'file:///home/bigdata-vm/Desktop/teste.txt'\n",
    "\n",
    "file_RDD = sc.textFile(file)\n",
    "print(file_RDD.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddfd91",
   "metadata": {},
   "source": [
    "# Rodando tarefas distribuídas\n",
    "\n",
    "A primeira coisa que você precisa saber sobre um RDD é que ele é **imutável**. Ou seja, para editar um valor de um RDD, você precisa criar um novo com o valor editado.\n",
    "\n",
    "Operações em RDDs são divididas em duas categorias:\n",
    "- _Transformações:_ constroem novos RDDs a partir de mudanças nos dados.\n",
    "- _Ações:_ realizam cálculos e retornam seus resultados para o programa _driver_.\n",
    "\n",
    "O Spark realiza uma avaliação **preguiçosa** (_lazy evaluation_) nas operações que precisa executar. Isso significa que ele irá _analisar_ as **transformações**, mas não irá executá-las de imediato. Ele apenas executa tudo quando uma **ação** é _executada_. A razão de ele fazer isso é que ele constrói um _grafo acíclico direcionado_ a partir das transformações, e prepara a maneira como esses dados serão processados no cluster de antemão. Quando uma ação é invocada, ele envia cópias da execução de todas as operações para os clusters, executa as transformações lá, e retorna com o resultado da ação invocada. \n",
    "\n",
    "Vamos fazer um simples exemplo de uma aplicação completa de uma tarefa em Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6d69b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.363668571647024, 6.094919414919659, 5.186442178104185]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# aplicação para contar elementos maiores que 5 em uma lista\n",
    "\n",
    "\n",
    "random_RDD = sc.parallelize([random.random() * 10 for _ in range(15)])\n",
    "\n",
    "def high_5(elem):\n",
    "    return elem > 5\n",
    "\n",
    "filtered_RDD = random_RDD.filter(high_5)\n",
    "print(filtered_RDD.take(3))\n",
    "print(filtered_RDD.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c273b9",
   "metadata": {},
   "source": [
    "# Transformações\n",
    "\n",
    "No exemplo acima vimos um exemplo de transformação (`filter`) e dois de ações (`take` e `count`). \n",
    "\n",
    "As transformações podem requerir _funções_ (sejam funções lambda ou explicitamente implementadas) para realizarem suas operações. As principais são:\n",
    "- `filter()`: filtra os elementos de um RDD de acordo com alguma condição expressa em uma função\n",
    "- `map()`: transforma todos os elementos de um RDD de acordo com alguma função (relacionamento 1 para 1)\n",
    "- `flatmap()`: \"explode\" os elementos de um RDD em múltiplos elementos de acordo com alguma função (relacionamento 1 para muitos)\n",
    "- `sample()`: retira uma amostra dos dados, com ou sem repetição, de acordo com uma semente randomica\n",
    "- `union()`: une dois RDDs\n",
    "\n",
    "Existem muitas outras transformações, listadas [na documentação oficial do Spark](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations). Existe um segundo grupo de transformações, que trabalha com **pares** de chave-valor (lembram do Hadoop?), que veremos mais a frente.\n",
    "\n",
    "Vamos fazer um exemplo de cada uma listada acima (já vimos `filter()` logo acima):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68be6056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 5, 16]\n"
     ]
    }
   ],
   "source": [
    "# exemplo: map()\n",
    "# contar caracteres em elementos lista de strings\n",
    "\n",
    "text_list = ['Esse','é','um','texto','que será contado']\n",
    "\n",
    "def count_char(elem):\n",
    "    return len(elem)\n",
    "\n",
    "character_count = sc.parallelize(text_list).map(lambda x: len(x)).collect()\n",
    "print(character_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16a4bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Esse é um teste Spark!']\n",
      "['Esse', 'é', 'um', 'teste', 'Spark!']\n"
     ]
    }
   ],
   "source": [
    "# exemplo: flatMap()\n",
    "# contar quantas palavras tem em um texto\n",
    "\n",
    "print(file_RDD.collect())\n",
    "exploded_file_RDD = file_RDD.flatMap(lambda x: x.split(' '))\n",
    "print(exploded_file_RDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc63c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5270845948766592, 0.6435380616469202, 0.19145380183510619, 0.1813743341318429, 0.39117156685207943, 0.3781987402353847, 0.1142125432650205, 0.9462571134089652, 0.582001283118308, 0.2211692447572009]\n",
      "390\n"
     ]
    }
   ],
   "source": [
    "# exemplo sample()\n",
    "# amostragem aleatória sem repetição\n",
    "\n",
    "sample_RDD = sc.parallelize([random.random() for _ in range(1000)])\n",
    "sample = sample_RDD.sample(withReplacement=False,fraction=0.4, seed=100).collect()\n",
    "print(sample[0:10])\n",
    "print(len(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8a62851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# exemplo union()\n",
    "# somar todos elementos de dois RDDs\n",
    "\n",
    "first_RDD = sc.parallelize([1,2,3,4,5])\n",
    "second_RDD = sc.parallelize([6,7,8,9,10])\n",
    "\n",
    "sum_elems = first_RDD.union(second_RDD).reduce(lambda x,y: x + y)\n",
    "print(sum_elems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a8dae",
   "metadata": {},
   "source": [
    "# Ações\n",
    "\n",
    "Enquanto vimos alguns exemplos de Transformações, aproveitamos para explorar algumas ações, como `collect()`, `count()` e `reduce`. Existem muitas outras opções além dessas. Vamos rever algumas que já utilizamos e visitar algumas novas:\n",
    "- `collect()`: Retorna todos os elementos de um RDD como uma lista para o programa driver.\n",
    "- `count()`: Conta o número de elementos que estão em um RDD, e retorna esse valor para o programa driver.\n",
    "- `reduce()`: reduz o RDD a um único valor com base em uma _função_. Essa função deve ser formatada de modo que recebe dois argumentos e retorna apenas um valor.\n",
    "- `take()`: recebe como parâmetro um valor _n_ e retorna uma quantidade de elementos igual a esse valor em formato de lista para o programa driver. Existe uma variação de retorno ordenado chamada `takeOrdered()`.\n",
    "- `takeSample()`: essa ação realiza operação similar à transformação `sample()`, retornando uma lista de elementos ao programa driver.\n",
    "- `saveAsTextFile()`: salva um RDD como arquivo de texto em um caminho especificado.\n",
    "\n",
    "Existem muitas outras ações, listadas [na documentação oficial do Spark](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions). Existe também uma ação baseada em pares chave-valor, que veremos depois de testarmos mais alguns exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c07d9c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3020167971883907, 0.5270845948766592, 0.9090322356086058, 0.08552566386052562, 0.6435380616469202]\n",
      "[0.0011751666205683797, 0.00198660733292344, 0.0030170825765947207, 0.003584232220728012, 0.004283218225352714]\n",
      "[0.6023323970672158, 0.18634818900509886, 0.929008964039758, 0.2629617511269403, 0.3760516333791669]\n"
     ]
    }
   ],
   "source": [
    "# exemplo take(), takeOrdered() e takeSample()\n",
    "\n",
    "print(sample_RDD.take(5))\n",
    "print(sample_RDD.takeOrdered(5))\n",
    "print(sample_RDD.takeSample(withReplacement=False, num=5, seed=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc32eadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n",
      "6\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# mais um exemplo reduce\n",
    "# encontrar o maior e o menor\n",
    "\n",
    "print(list_RDD.collect())\n",
    "\n",
    "def max_func(a,b):\n",
    "    if a > b:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "print(list_RDD.reduce(max_func))\n",
    "print(list_RDD.reduce(lambda a,b: a if (a < b) else b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06d486c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Esse é um teste Spark!']\n"
     ]
    }
   ],
   "source": [
    "# exemplo saveAsTextFile()\n",
    "# salvar arquivo com 10 linhas repetidas\n",
    "\n",
    "print(file_RDD.collect())\n",
    "file_RDD.flatMap(lambda x: [x for _ in range(10)]).saveAsTextFile('file:///home/bigdata-vm/Desktop/teste2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374bbf14",
   "metadata": {},
   "source": [
    "# RDDs como pares chave-valor\n",
    "\n",
    "Nós conseguimos simular as operações de MapReduce no Spark utilizando transformações e ações que operam em pares chave-valor. Na realidade, nós temos até um pouco mais de liberdade do que o MapReduce tradicional aqui. Para operar com chaves, o Spark oferece as seguintes transformações:\n",
    "\n",
    "- `groupByKey()`: retorna um RDD no formato (K, Iterable\\<V\\>).\n",
    "- `reduceByKey()`: \tretorna um RDD de formato (K, V), onde os valores de cada chave foram reduzidos por alguma _função_, que deve receber dois valores como parâmetros e retornar um único valor. \n",
    "- `aggregateByKey()`: generalização do `reduceByKey()`. Aqui podemos informar um valor de início da agregação, e definir duas funções: uma que combina os resultados em uma partição, e outra que reduz o resultado da segunda no programa driver.\n",
    "- `sortByKey()`: retorna um RDD ordenado pelas chaves em ordem ascendente ou descendente.\n",
    "\n",
    "O Spark oferece uma única ação baseada em chaves:\n",
    "\n",
    "- `countByKey()`: retorna um dicionário com a contagem de ocorrências das chaves em um RDD.\n",
    "\n",
    "Vamos ver alguns exemplos com o mesmo conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2578001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', <pyspark.resultiterable.ResultIterable object at 0x7f8a2c34c700>), ('c', <pyspark.resultiterable.ResultIterable object at 0x7f8a2c34c460>), ('a', <pyspark.resultiterable.ResultIterable object at 0x7f8a2c34c970>), ('d', <pyspark.resultiterable.ResultIterable object at 0x7f8a2c34c910>)]\n",
      "b [1, 2]\n",
      "c [1, 2]\n",
      "a [1, 2, 3, 4]\n",
      "d [1]\n"
     ]
    }
   ],
   "source": [
    "# exemplo groupByKey()\n",
    "\n",
    "tuple_list = [('a',1),('a',2),('a',3),('b',1),('c',1),('a',4),('b',2),('d',1),('c',2)]\n",
    "tuple_RDD = sc.parallelize(tuple_list)\n",
    "\n",
    "grouped_elements = tuple_RDD.groupByKey().collect()\n",
    "print(grouped_elements)\n",
    "for key, values in grouped_elements:\n",
    "        print(key, list(values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c16bba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 3), ('c', 3), ('a', 10), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "#exemplo reduceByKey()\n",
    "\n",
    "reduced_elements = tuple_RDD.reduceByKey(lambda a,b: a+b).collect()\n",
    "print(reduced_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0cbab6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 2), ('c', 2), ('a', 4), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "# exemplo aggregateByKey()\n",
    "\n",
    "agg_elements = tuple_RDD.aggregateByKey(0, lambda a,b: a+b, lambda a,b: a if (a>b) else b)\n",
    "print(agg_elements.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d7bea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 2), ('a', 3), ('a', 4), ('b', 1), ('b', 2), ('c', 1), ('c', 2), ('d', 1)]\n",
      "[('d', 1), ('c', 1), ('c', 2), ('b', 1), ('b', 2), ('a', 1), ('a', 2), ('a', 3), ('a', 4)]\n",
      "a [1, 2, 3, 4]\n",
      "b [1, 2]\n",
      "c [1, 2]\n",
      "d [1]\n"
     ]
    }
   ],
   "source": [
    "# exemplo sortByKey()\n",
    "\n",
    "print(tuple_RDD.sortByKey(ascending=True).collect())\n",
    "print(tuple_RDD.sortByKey(ascending=False).collect())\n",
    "\n",
    "sorted_grouped_RDD = tuple_RDD.groupByKey().sortByKey(ascending=True)\n",
    "for key, values in sorted_grouped_RDD.collect():\n",
    "    print(key, list(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15f74dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'a': 4, 'b': 2, 'c': 2, 'd': 1})\n"
     ]
    }
   ],
   "source": [
    "#exemplo countByKey()\n",
    "\n",
    "print(tuple_RDD.countByKey())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b9b40",
   "metadata": {},
   "source": [
    "# Aplicações completas com RDDs\n",
    "\n",
    "Agora que sabemos as principais funcionalidades com RDDs, vamos fazer alguns exemplos de aplicações com RDDs. Vamos fazer os mesmos exemplos que vimos em MapReduce. \n",
    "\n",
    "- Contagem de palavras com NLTK\n",
    "- Amigos em comum\n",
    "- Minutos de atraso por aeroporto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92ef6b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 44), ('8', 2), ('9', 5), ('10', 6), ('dramatis', 37)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# contagem de palavras\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "shakespeare_file = 'file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/shakespeare.txt'\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def tokenizer(line):\n",
    "    for token in nltk.word_tokenize(line):\n",
    "        if token not in stopwords and token not in punctuation:\n",
    "            yield token.lower()\n",
    "\n",
    "shakespeare_RDD = sc.textFile(shakespeare_file)\n",
    "tokens_RDD = shakespeare_RDD.flatMap(tokenizer)\n",
    "key_tokens_RDD = tokens_RDD.map(lambda x: (x,1))\n",
    "token_counts = key_tokens_RDD.reduceByKey(lambda a,b: a+b)\n",
    "print(token_counts.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ee1e3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Allen', 'Betty'), {'Chris', 'Betty', 'David'}), (('Allen', 'Betty'), {'Ellen', 'Chris', 'David', 'Allen'}), (('Allen', 'Chris'), {'Chris', 'Betty', 'David'}), (('Allen', 'Chris'), {'Ellen', 'Betty', 'David', 'Allen'})]\n",
      "[(('Allen', 'David'), {'Chris', 'Betty'})]\n"
     ]
    }
   ],
   "source": [
    "# amigos em comum\n",
    "\n",
    "friends = 'file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/RelatedFriends.txt'\n",
    "friends_RDD = sc.textFile(friends)\n",
    "\n",
    "def friends_map(line):\n",
    "    key, values = line.split(':') # 'Allen', 'Betty, Chris, David'\n",
    "    values = [value.strip() for value in values.split(',')] # ['Betty','Chris', 'David']\n",
    "    pair_list = []\n",
    "    \n",
    "    for value in values:\n",
    "        pair = [key, value]\n",
    "        pair.sort()\n",
    "        pair_list.append((tuple(pair), set(values))) # [(('Allen', 'Betty'), set('Betty','Chris', 'David'))]\n",
    "    \n",
    "    return pair_list\n",
    "        \n",
    "key_friends_RDD = friends_RDD.flatMap(friends_map)\n",
    "print(key_friends_RDD.sortByKey().take(4))\n",
    "\n",
    "def friends_reduce(pair1, pair2):\n",
    "    return pair1.intersection(pair2)\n",
    "\n",
    "mutual_friends = key_friends_RDD.reduceByKey(lambda pair1,pair2: pair1.intersection(pair2)).take(1)\n",
    "print(mutual_friends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a4a436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LAX', 149470.0), ('BOS', 49478.0), ('SFO', 124901.0), ('ATL', 261924.0), ('DCA', 27099.0), ('ORD', 274709.0), ('PBI', 26107.0), ('FLL', 63583.0), ('IAD', 54591.0), ('SJU', 12262.0)]\n"
     ]
    }
   ],
   "source": [
    "# minutos de atraso por aeroporto\n",
    "\n",
    "flights_file = 'file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/flights.csv'\n",
    "flights_RDD = sc.textFile(flights_file)\n",
    "\n",
    "def read_line(line):\n",
    "    record = line.split(',')\n",
    "    return (record[3],float(record[6]))\n",
    "\n",
    "delay_RDD = flights_RDD.map(read_line)\n",
    "print(delay_RDD.reduceByKey(lambda a,b: a+b).take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64423b06",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Vamos finalizar fazendo um exemplo de uma aplicação um pouco mais complexa. O TF-IDF basicamente indexa palavras e documentos utilizando chaves da forma `(token, doc)` e valores representando uma medida que relaciona frequência do termo em um documento com a raridade dele na coleção.\n",
    "\n",
    "Em Hadoop vimos que era necessário três execuções de MapReduce, uma em seguida da outra, para realizar a indexação. Aqui segue _quase_ o mesmo princípio.\n",
    "\n",
    "- 1. Realizamos o mapeamento de palavras com documentos\n",
    "- 2. Calculamos o TF de cada elemento\n",
    "- 3. Em um _segundo_ RDD, calculamos o DF, e depois mapeamos para o IDF\n",
    "- 4. Finalmente, unimos os dois RDDs, e reduzimos com uma função de multiplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e83915e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('file:/home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/reuters-subset/subset/9', 'voted'), 3.713572066704308), (('file:/home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/reuters-subset/subset/9', 'two-for-one'), 3.713572066704308), (('file:/home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/reuters-subset/subset/9', 'the'), 1)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "reuters_dir = 'file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/reuters-subset/subset/'\n",
    "documents_RDD = sc.wholeTextFiles(reuters_dir)\n",
    "\n",
    "# recuperar quantidade de documentos\n",
    "N = documents_RDD.count()\n",
    "\n",
    "# mapear em ((doc_id,token), 1)\n",
    "def document_map(doc):\n",
    "    tokens_list = [token for token in tokenizer(doc[1])]\n",
    "    return map(lambda token: ((doc[0], token), 1), tokens_list)\n",
    "\n",
    "doc_tokens_RDD = documents_RDD.flatMap(document_map)\n",
    "\n",
    "# calcular TF\n",
    "tf_RDD = doc_tokens_RDD.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "#calcular DF\n",
    "def count_df(agg, value):\n",
    "    return (value[0], agg[1]+1)\n",
    "\n",
    "#  x[0][0] x[0][1] x[1]\n",
    "# ((doc_id,token), tf) => (token, (doc_id, df))\n",
    "df_RDD = tf_RDD.map(lambda x: (x[0][1], (x[0][0], x[1]))).reduceByKey(count_df)\n",
    "\n",
    "#calcular idf\n",
    "def calc_idf(elem):\n",
    "    if elem[1] != 0:\n",
    "        return (elem[0], math.log(N/elem[1]))\n",
    "    else:\n",
    "        return elem\n",
    "\n",
    "\n",
    "#    x[0]  x[1][0]  x[1][1]\n",
    "# (token, (doc_id, df)) => ((doc_id,token), df)\n",
    "idf_RDD = df_RDD.map(lambda x: ((x[1][0],x[0]), x[1][1])).map(calc_idf)\n",
    "\n",
    "#calcular tf*idf\n",
    "tf_idf_RDD = tf_RDD.union(idf_RDD).reduceByKey(lambda a,b: a*b)\n",
    "print(tf_idf_RDD.sortByKey(ascending=False).take(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c2358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
