{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4545be",
   "metadata": {},
   "source": [
    "# Aprendizado de Máquina com Spark\n",
    "\n",
    "Spark oferece um ambiente completo para aprendizado de máquina em sua biblioteca `MLlib`, que implementa diversas tarefas de modo distribuído e escalável. Neste notebook veremos alguns exemplos de suas funcionalidades, que podem ser divididas em três grandes categorias:\n",
    "- Transformações de Características\n",
    "- Algoritmos\n",
    "- Otimização \n",
    "\n",
    "Vamos começar importando as bibliotecas do Spark e inicializando uma `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197560c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 09:29:07,215 WARN util.Utils: Your hostname, bigdatavm-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "2021-12-18 09:29:07,256 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-12-18 09:29:08,355 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f6e7e057d30>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8526455f",
   "metadata": {},
   "source": [
    "## Transformações de Características\n",
    "\n",
    "Spark oferece uma grande quantidade de transformações de características que podem ser aplicadas em Dataframes. Essas transformações vão além das funcionalidades vistas antes na biblioteca SQL. \n",
    "\n",
    "\n",
    "As transformações de características estão localizadas no módulo `pyspark.ml.feature`. Podemos checar todas as funcionalidades na [documentação](https://spark.apache.org/docs/latest/ml-features.html). No caso deste notebook, veremos as cinco mais comuns:\n",
    "- Indexador de String\n",
    "- Transformador de representações OneHot\n",
    "- Transformador de valores em _buckets_\n",
    "- TF-IDF\n",
    "- Criador de vetores\n",
    "\n",
    "\n",
    "Mas antes de vermos eles, vamos importar e preparar as bases de dados `flights.csv` e `sms.csv`, usando SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f44fa",
   "metadata": {},
   "source": [
    "Preparando `flights.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15f28b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n",
      "10000\n",
      "9925\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# importando e olhando o Schema\n",
    "flights_df = spark.read.csv('file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/Flights_new/flights.csv',\n",
    "                           header = True, inferSchema=True, nullValue='NA')\n",
    "flights_df.printSchema()\n",
    "\n",
    "# definindo a coluna alvo\n",
    "flights_df = flights_df.withColumn('Y_num', flights_df.arr_delay)\n",
    "flights_df = flights_df.withColumn('Y_bin', F.when(flights_df['arr_delay'] > 5, 1).otherwise(0))\n",
    "\n",
    "# normalizando valores por min-max\n",
    "columns_to_scale = ['dep_delay', 'distance']\n",
    "\n",
    "def min_max_norm(df, cols):\n",
    "    for col in cols:\n",
    "        max_value = df.agg({col:'max'}).collect()[0][0]\n",
    "        min_value = df.agg({col:'min'}).collect()[0][0]\n",
    "        new_column = col+'_scaled'\n",
    "        df = df.withColumn(new_column, (df[col] - min_value) / (max_value - min_value))\n",
    "    return df\n",
    "\n",
    "flights_df = min_max_norm(flights_df, columns_to_scale)\n",
    "\n",
    "# remover colunas desnecessarias\n",
    "flights_df = flights_df.drop(*['year','month','day','dep_time','dep_delay', 'arr_time',\n",
    "                              'arr_delay','tailnum', 'flight', 'air_time', 'minute'])\n",
    "\n",
    "#remover linhas com valores nulos\n",
    "print(flights_df.count())\n",
    "flights_df = flights_df.dropna()\n",
    "print(flights_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7f0410",
   "metadata": {},
   "source": [
    "Preparando `sms.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee7fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      "\n",
      "+--------------------+---+\n",
      "|                text|  Y|\n",
      "+--------------------+---+\n",
      "|Sorry, I'll call ...|  0|\n",
      "|Dont worry. I gue...|  0|\n",
      "|Call FREEPHONE 08...|  1|\n",
      "|Win a 1000 cash p...|  1|\n",
      "|Go until jurong p...|  0|\n",
      "|Ok lar... Joking ...|  0|\n",
      "|Free entry in 2 a...|  1|\n",
      "|U dun say so earl...|  0|\n",
      "|Nah I don't think...|  0|\n",
      "|FreeMsg Hey there...|  1|\n",
      "+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importando e olhando o Schema\n",
    "sms_df = spark.read.csv('file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/sms.csv', \n",
    "                        inferSchema=True, sep=';')\n",
    "sms_df.printSchema()\n",
    "\n",
    "# renomear colunas\n",
    "sms_df = sms_df.withColumn('text', sms_df._c1).drop('_c1')\n",
    "sms_df = sms_df.withColumn('Y', sms_df._c2).drop(*['_c0','_c2'])\n",
    "sms_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d073b",
   "metadata": {},
   "source": [
    "### StringIndexer\n",
    "\n",
    "O Indexador de String transforma o conteúdo de cada célula de uma coluna de Strings em um valor categórico. Essa decisão é feita com base na frequência do elemento. \n",
    "\n",
    "Vamos modificar algumas colunas de `flights_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "165a2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checando se \"carrier\" é categórico\n",
      "+-------+\n",
      "|carrier|\n",
      "+-------+\n",
      "|     UA|\n",
      "|     AA|\n",
      "|     B6|\n",
      "|     DL|\n",
      "|     OO|\n",
      "|     F9|\n",
      "|     US|\n",
      "|     HA|\n",
      "|     AS|\n",
      "|     VX|\n",
      "|     WN|\n",
      "+-------+\n",
      "\n",
      "+-------+-----+\n",
      "|carrier|count|\n",
      "+-------+-----+\n",
      "|     UA| 1039|\n",
      "|     AA|  476|\n",
      "|     B6|  213|\n",
      "|     DL| 1077|\n",
      "|     OO| 1163|\n",
      "|     F9|  181|\n",
      "|     US|  364|\n",
      "|     HA|   71|\n",
      "|     AS| 3771|\n",
      "|     VX|  186|\n",
      "|     WN| 1384|\n",
      "+-------+-----+\n",
      "\n",
      "Indexando \"carrier\"\n",
      "+-------+-----------+\n",
      "|carrier|carrier_idx|\n",
      "+-------+-----------+\n",
      "|     UA|        4.0|\n",
      "|     WN|        1.0|\n",
      "|     AS|        0.0|\n",
      "|     HA|       10.0|\n",
      "|     US|        6.0|\n",
      "|     AA|        5.0|\n",
      "|     B6|        7.0|\n",
      "|     OO|        2.0|\n",
      "|     VX|        8.0|\n",
      "|     DL|        3.0|\n",
      "|     F9|        9.0|\n",
      "+-------+-----------+\n",
      "\n",
      "Indexando \"origin\" e \"dest\"\n",
      "+------+----------+\n",
      "|origin|origin_idx|\n",
      "+------+----------+\n",
      "|   PDX|       1.0|\n",
      "|   SEA|       0.0|\n",
      "+------+----------+\n",
      "\n",
      "+----+--------+\n",
      "|dest|dest_idx|\n",
      "+----+--------+\n",
      "| BLI|    65.0|\n",
      "| FAT|    41.0|\n",
      "| EWR|    20.0|\n",
      "| SIT|    64.0|\n",
      "| BWI|    47.0|\n",
      "| LGB|    17.0|\n",
      "| HOU|    54.0|\n",
      "| RDM|    38.0|\n",
      "| BUR|    23.0|\n",
      "| CVG|    66.0|\n",
      "| OAK|    10.0|\n",
      "| CLT|    33.0|\n",
      "| MDW|    30.0|\n",
      "| MCO|    49.0|\n",
      "| LIH|    46.0|\n",
      "| MIA|    56.0|\n",
      "| HNL|    21.0|\n",
      "| MSP|    14.0|\n",
      "| LMT|    63.0|\n",
      "| TUS|    40.0|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Checando se \"carrier\" é categórico')\n",
    "flights_df.select('carrier').distinct().show()\n",
    "flights_df.groupBy('carrier').count().show()\n",
    "\n",
    "print('Indexando \"carrier\"')\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "indexer_model = indexer.fit(flights_df)\n",
    "flights_si = indexer_model.transform(flights_df)\n",
    "flights_si.select('carrier', 'carrier_idx').distinct().show()\n",
    "\n",
    "print('Indexando \"origin\" e \"dest\"')\n",
    "indexer = StringIndexer(inputCols=['origin', 'dest'], outputCols= ['origin_idx', 'dest_idx'])\n",
    "flights_si = indexer.fit(flights_si).transform(flights_si)\n",
    "flights_si.select('origin', 'origin_idx').distinct().show()\n",
    "flights_si.select('dest', 'dest_idx').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d19cf",
   "metadata": {},
   "source": [
    "### OneHotEncoder\n",
    "\n",
    "Dados categóricos não podem ser manipulados pela maioria de algoritmos de aprendizado de máquina, pelo simples fato de que eles não possuem relação matemática alguma entre si. Para podermos usar dados categóricos na maioria dos algoritmos, precisamos transformá-los em uma representação **one-hot**. \n",
    "\n",
    "Spark realiza essa conversão através do `OneHotEncoder`, porém o que faz de fato é gerar uma representação **dummy**: \n",
    "\n",
    "![Representação one-hot. Retirado de: https://www.kaggle.com/getting-started/187540](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5315434%2Fa9886ea90db74aad0b2f86d2686c337b%2Fohe-vs-dummy.png?generation=1601465979026694&alt=media)\n",
    "\n",
    "Vamos converter os índices que criamos em representações one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0fbf5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando one-hots para \"carrier_idx\"\n",
      "+-----------+--------------+\n",
      "|carrier_idx|    carrier_oh|\n",
      "+-----------+--------------+\n",
      "|        6.0|(10,[6],[1.0])|\n",
      "|        9.0|(10,[9],[1.0])|\n",
      "|        8.0|(10,[8],[1.0])|\n",
      "|        0.0|(10,[0],[1.0])|\n",
      "|        3.0|(10,[3],[1.0])|\n",
      "|        2.0|(10,[2],[1.0])|\n",
      "|        7.0|(10,[7],[1.0])|\n",
      "|       10.0|    (10,[],[])|\n",
      "|        1.0|(10,[1],[1.0])|\n",
      "|        4.0|(10,[4],[1.0])|\n",
      "|        5.0|(10,[5],[1.0])|\n",
      "+-----------+--------------+\n",
      "\n",
      "Criando one-hots para \"origin\" e \"dest\"\n",
      "+----------+-------------+\n",
      "|origin_idx|    origin_oh|\n",
      "+----------+-------------+\n",
      "|       1.0|    (1,[],[])|\n",
      "|       0.0|(1,[0],[1.0])|\n",
      "+----------+-------------+\n",
      "\n",
      "+--------+---------------+\n",
      "|dest_idx|        dest_oh|\n",
      "+--------+---------------+\n",
      "|     4.0| (68,[4],[1.0])|\n",
      "|    17.0|(68,[17],[1.0])|\n",
      "|    51.0|(68,[51],[1.0])|\n",
      "|     3.0| (68,[3],[1.0])|\n",
      "|    27.0|(68,[27],[1.0])|\n",
      "|    10.0|(68,[10],[1.0])|\n",
      "|    44.0|(68,[44],[1.0])|\n",
      "|    16.0|(68,[16],[1.0])|\n",
      "|    58.0|(68,[58],[1.0])|\n",
      "|    49.0|(68,[49],[1.0])|\n",
      "|    33.0|(68,[33],[1.0])|\n",
      "|    43.0|(68,[43],[1.0])|\n",
      "|    64.0|(68,[64],[1.0])|\n",
      "|    34.0|(68,[34],[1.0])|\n",
      "|    25.0|(68,[25],[1.0])|\n",
      "|    20.0|(68,[20],[1.0])|\n",
      "|    39.0|(68,[39],[1.0])|\n",
      "|    13.0|(68,[13],[1.0])|\n",
      "|     6.0| (68,[6],[1.0])|\n",
      "|    24.0|(68,[24],[1.0])|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "print('Criando one-hots para \"carrier_idx\"')\n",
    "onehot = OneHotEncoder(inputCol='carrier_idx', outputCol='carrier_oh')\n",
    "flights_oh = onehot.fit(flights_si).transform(flights_si)\n",
    "flights_oh.select('carrier_idx', 'carrier_oh').distinct().show()\n",
    "\n",
    "print('Criando one-hots para \"origin\" e \"dest\"')\n",
    "onehot = OneHotEncoder(inputCols=['origin_idx', 'dest_idx'], outputCols=['origin_oh', 'dest_oh'])\n",
    "flights_oh = onehot.fit(flights_oh).transform(flights_oh)\n",
    "flights_oh.select('origin_idx', 'origin_oh').distinct().show()\n",
    "flights_oh.select('dest_idx', 'dest_oh').distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90baa6b",
   "metadata": {},
   "source": [
    "### Bucketizer\n",
    "\n",
    "Às vezes é interessante transformar valores contínuos em discretos para uma generalização melhor do modelo. O modelo assim passa a diferenciar por _categorias_ de valores e não tentar entender um comportamento a partir da variação em um valor. Ou ainda, é interessante diminuir o número de categorias presentes em uma variável já discreta.\n",
    "\n",
    "Essa tarefa é conhecida como _bucketing_ ou _binning_. Vamos fazer essa transformação com a coluna `'hour'`, e produzir vetores one-hot a partir dela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "339b5774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ver valores únicos de \"hour\"\n",
      "+----+\n",
      "|hour|\n",
      "+----+\n",
      "|  12|\n",
      "|  22|\n",
      "|   1|\n",
      "|  13|\n",
      "|   6|\n",
      "|  16|\n",
      "|  20|\n",
      "|   5|\n",
      "|  19|\n",
      "|  15|\n",
      "|  17|\n",
      "|   9|\n",
      "|   8|\n",
      "|  23|\n",
      "|   7|\n",
      "|  10|\n",
      "|  24|\n",
      "|  21|\n",
      "|  11|\n",
      "|  14|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Reduzir para buckets de 3 horas\n",
      "+----+-----------+\n",
      "|hour|hour_bucket|\n",
      "+----+-----------+\n",
      "|  23|        7.0|\n",
      "|  15|        5.0|\n",
      "|  12|        4.0|\n",
      "|  22|        7.0|\n",
      "|   8|        2.0|\n",
      "|   1|        0.0|\n",
      "|  21|        7.0|\n",
      "|   9|        3.0|\n",
      "|   0|        0.0|\n",
      "|  10|        3.0|\n",
      "|   5|        1.0|\n",
      "|  13|        4.0|\n",
      "|  24|        7.0|\n",
      "|  18|        6.0|\n",
      "|  17|        5.0|\n",
      "|  16|        5.0|\n",
      "|   7|        2.0|\n",
      "|   2|        0.0|\n",
      "|  20|        6.0|\n",
      "|  11|        3.0|\n",
      "+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-----------+-------------+\n",
      "|hour|hour_bucket|      hour_oh|\n",
      "+----+-----------+-------------+\n",
      "|   1|        0.0|(7,[0],[1.0])|\n",
      "|   8|        2.0|(7,[2],[1.0])|\n",
      "|   5|        1.0|(7,[1],[1.0])|\n",
      "|  12|        4.0|(7,[4],[1.0])|\n",
      "|  20|        6.0|(7,[6],[1.0])|\n",
      "|   7|        2.0|(7,[2],[1.0])|\n",
      "|   0|        0.0|(7,[0],[1.0])|\n",
      "|  22|        7.0|    (7,[],[])|\n",
      "|  21|        7.0|    (7,[],[])|\n",
      "|  18|        6.0|(7,[6],[1.0])|\n",
      "|  10|        3.0|(7,[3],[1.0])|\n",
      "|   2|        0.0|(7,[0],[1.0])|\n",
      "|  15|        5.0|(7,[5],[1.0])|\n",
      "|  19|        6.0|(7,[6],[1.0])|\n",
      "|  13|        4.0|(7,[4],[1.0])|\n",
      "|  14|        4.0|(7,[4],[1.0])|\n",
      "|  11|        3.0|(7,[3],[1.0])|\n",
      "|  24|        7.0|    (7,[],[])|\n",
      "|   9|        3.0|(7,[3],[1.0])|\n",
      "|  17|        5.0|(7,[5],[1.0])|\n",
      "+----+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Ver valores únicos de \"hour\"')\n",
    "flights_oh.select('hour').distinct().show()\n",
    "\n",
    "print('Reduzir para buckets de 3 horas')\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucket = Bucketizer(splits=[0,3,6,9,12,15,18,21,24], inputCol='hour', outputCol='hour_bucket')\n",
    "flights_bucket = bucket.transform(flights_oh)\n",
    "flights_bucket.select('hour','hour_bucket').distinct().show()\n",
    "\n",
    "onehot = OneHotEncoder(inputCol='hour_bucket', outputCol='hour_oh')\n",
    "flights_bucket = onehot.fit(flights_bucket).transform(flights_bucket)\n",
    "flights_bucket.select('hour','hour_bucket', 'hour_oh').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf240b1",
   "metadata": {},
   "source": [
    "### VectorAssembler\n",
    "\n",
    "Spark requer que toda a informação que será passada para um algoritmo de ML seja convertida em um único vetor. Para fazer isso, usaremos `VectorAssembler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc8e453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- Y_num: integer (nullable = true)\n",
      " |-- Y_bin: integer (nullable = false)\n",
      " |-- dep_delay_scaled: double (nullable = true)\n",
      " |-- distance_scaled: double (nullable = true)\n",
      " |-- carrier_idx: double (nullable = false)\n",
      " |-- origin_idx: double (nullable = false)\n",
      " |-- dest_idx: double (nullable = false)\n",
      " |-- carrier_oh: vector (nullable = true)\n",
      " |-- origin_oh: vector (nullable = true)\n",
      " |-- dest_oh: vector (nullable = true)\n",
      " |-- hour_bucket: double (nullable = true)\n",
      " |-- hour_oh: vector (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------------------+-----+-----+\n",
      "|features                                                                        |Y_num|Y_bin|\n",
      "+--------------------------------------------------------------------------------+-----+-----+\n",
      "|(88,[0,1,10,12,14,83],[0.013259668508287293,0.3272519954389966,1.0,1.0,1.0,1.0])|-5   |0    |\n",
      "|(88,[0,1,2,12,34,84],[0.026519337016574586,0.9821360699353858,1.0,1.0,1.0,1.0]) |5    |0    |\n",
      "|(88,[0,1,10,12,13,85],[0.01878453038674033,0.2227290003800836,1.0,1.0,1.0,1.0]) |2    |0    |\n",
      "|(88,[0,1,3,22,86],[0.0707182320441989,0.18091980235651844,1.0,1.0,1.0])         |34   |1    |\n",
      "|(88,[0,1,2,12,36,83],[0.019889502762430938,0.32079057392626376,1.0,1.0,1.0,1.0])|1    |0    |\n",
      "|(88,[0,1,3,15,84],[0.028729281767955802,0.3413150893196503,1.0,1.0,1.0])        |2    |0    |\n",
      "|(88,[0,1,3,23,83],[0.06740331491712707,0.17103762827822122,1.0,1.0,1.0])        |51   |1    |\n",
      "|(88,[0,1,10,12,13,86],[0.015469613259668509,0.2227290003800836,1.0,1.0,1.0,1.0])|-18  |0    |\n",
      "|(88,[0,1,2,12,25,85],[0.016574585635359115,0.3637400228050171,1.0,1.0,1.0,1.0]) |-7   |0    |\n",
      "|(88,[0,1,2,12,19,87],[0.017679558011049725,0.6187761307487647,1.0,1.0,1.0,1.0]) |-4   |0    |\n",
      "|(88,[0,1,2,12,14,86],[0.01878453038674033,0.3272519954389966,1.0,1.0,1.0,1.0])  |-1   |0    |\n",
      "|(88,[0,1,2,12,16,84],[0.020994475138121547,0.38540478905359177,1.0,1.0,1.0,1.0])|2    |0    |\n",
      "|(88,[0,1,2,12,17,83],[0.04419889502762431,0.29418472063854045,1.0,1.0,1.0,1.0]) |29   |1    |\n",
      "|(88,[0,1,2,12,18],[0.016574585635359115,0.5150133029266438,1.0,1.0,1.0])        |-28  |0    |\n",
      "|(88,[0,1,2,12,13,85],[0.11933701657458563,0.2227290003800836,1.0,1.0,1.0,1.0])  |111  |1    |\n",
      "|(88,[0,1,6,13,87],[0.02430939226519337,0.17369821360699353,1.0,1.0,1.0])        |9    |1    |\n",
      "|(88,[0,1,2,12,24,87],[0.07624309392265194,0.19460281261877613,1.0,1.0,1.0,1.0]) |41   |1    |\n",
      "|(88,[0,1,3,12,43,84],[0.017679558011049725,0.6233371341695173,1.0,1.0,1.0,1.0]) |-7   |0    |\n",
      "|(88,[0,1,2,12,37],[0.011049723756906077,0.9133409350057012,1.0,1.0,1.0])        |-15  |0    |\n",
      "|(88,[0,1,4,36,86],[0.0077348066298342545,0.2751805397187381,1.0,1.0,1.0])       |-10  |0    |\n",
      "+--------------------------------------------------------------------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "flights_bucket.printSchema()\n",
    "cols_to_use = ['dep_delay_scaled', 'distance_scaled', 'carrier_oh', 'origin_oh', 'dest_oh', 'hour_oh']\n",
    "\n",
    "vec = VectorAssembler(inputCols=cols_to_use, outputCol='features')\n",
    "flights_vec = vec.transform(flights_bucket)\n",
    "flights_vec.select('features', 'Y_num', 'Y_bin').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52af92f",
   "metadata": {},
   "source": [
    "### TF-IDF ...de novo\n",
    "\n",
    "Spark oferece funções para a criação de vetores TF-IDF. O processo é quebrado em três transformações: `Tokenizer`, `HashingTF`, `IDF`. Vamos transformar a coluna `'text'` em TF-IDF em nosso `sms_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbb78b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpando o texto - remover pontuação, números e espaços adicionais\n",
      "Tokenizando e removendo stop words\n",
      "Calculando TF\n",
      "Calculando TF-IDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 152:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tokens_clean                                                                                                                                 |features                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[sorry, call, later, meeting]                                                                                                                |(1024,[138,384,577,996],[2.273418200008753,3.6288353225642043,3.5890949939146903,4.104259019279279])                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|[dont, worry, guess, busy]                                                                                                                   |(1024,[215,233,276,329],[3.9913186080986836,3.3790235241678332,4.734227298217693,4.58299632849377])                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|[call, freephone]                                                                                                                            |(1024,[133,138],[5.367951058306837,2.273418200008753])                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|[win, cash, prize, prize, worth]                                                                                                             |(1024,[31,47,62,389],[3.6632029660684124,4.754846585420428,4.072170704727778,7.064594791043114])                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                          |(1024,[12,171,191,195,283,363,384,400,420,474,480,618,667,688,750,782],[4.962485950198674,4.674803877746892,4.482912869936787,2.9596209082158875,3.4442640460362344,5.853458874088538,3.6288353225642043,5.192060391843174,3.5761915890787823,5.258751766341845,3.6702205387270586,2.9984264826376825,5.792834252272104,4.864847480634757,3.027625637329945,3.2278848948105665])                                                                          |\n",
      "|[ok, lar, joking, wif, u, oni]                                                                                                               |(1024,[3,493,565,583,653,726],[4.937168142214383,5.581525158604896,2.7371696379954384,1.835950360814415,4.001074783044048,4.244020961654438])                                                                                                                                                                                                                                                                                                             |\n",
      "|[free, entry, wkly, comp, win, fa, cup, final, tkts, st, may, text, fa, receive, entry, question, std, txt, rate, t&c's, apply, 's]          |(1024,[16,24,35,56,62,77,131,193,213,217,314,332,429,478,501,587,625,694,880,985],[3.317779898927115,5.070699534838906,4.841857962410058,4.406539891152213,4.072170704727778,4.269338769638728,3.3790235241678332,3.096618508816896,5.367951058306837,2.9561666733478,7.797319555231958,5.160311693528593,3.342843867590331,4.335588155179928,8.041754820680456,5.160311693528593,3.38960563349837,4.363367719287004,4.797406199839225,4.195230797485006])|\n",
      "|[u, dun, say, early, hor, u, c, already, say]                                                                                                |(1024,[44,168,210,214,245,258,583],[4.637063549764045,3.9913186080986836,5.49055338039917,7.8878327384082,3.872457405221955,4.115188089811469,3.67190072162883])                                                                                                                                                                                                                                                                                          |\n",
      "|[nah, think, goes, usf, lives, around, though]                                                                                               |(1024,[14,364,709,761,803,847,1001],[5.293843086153116,3.9255672305359033,4.207206988531722,4.421354976937353,3.934699714099176,3.7207728178898902,4.5656045857819])                                                                                                                                                                                                                                                                                      |\n",
      "|[freemsg, hey, darling, week's, word, back, like, fun, still, tb, ok, xxx, std, chgs, send, rcv]                                             |(1024,[41,112,163,174,190,316,386,479,488,559,565,587,684,816,853,963],[5.407171771460119,3.3790235241678332,3.255409568200657,3.5508737810944924,4.797406199839225,3.813863240955902,3.1166592597003424,4.797406199839225,3.805766030723283,4.436392854301894,2.7371696379954384,5.160311693528593,4.841857962410058,2.6938024088803085,5.853458874088538,4.32198250312415])                                                                             |\n",
      "|[even, brother, like, speak, treat, like, aids, patent]                                                                                      |(1024,[197,318,319,386,579,732,886],[6.546606054648484,4.6557556827761974,3.4785531195148662,6.233318519400685,4.5151737321550085,4.5151737321550085,4.39194109173106])                                                                                                                                                                                                                                                                                   |\n",
      "|[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, press, *, copy, friends, callertune]           |(1024,[75,207,362,400,413,463,510,622,666,684,754,818,962,1023],[7.627726481911804,5.258751766341845,3.8638736615305636,5.192060391843174,4.282242174474636,5.447993765980374,4.3775523542789605,5.447993765980374,12.456304647059898,4.841857962410058,4.308559482792009,2.548405352979285,5.49055338039917,4.269338769638728])                                                                                                                          |\n",
      "|[winner, valued, network, customer, selected, receivea, prize, reward, claim, call, claim, code, kl, valid, hours]                           |(1024,[10,37,100,138,142,314,380,389,391,483,626,681,704,994],[4.734227298217693,4.421354976937353,4.674803877746892,2.273418200008753,5.917997395226109,3.898659777615979,4.734227298217693,3.532297395521557,5.49055338039917,4.819385106557999,6.228152323529949,7.4415456357797805,3.582622479409073,4.195230797485006])                                                                                                                              |\n",
      "|[mobile, months, u, r, entitled, update, latest, colour, mobiles, camera, free, call, mobile, update, co, free]                              |(1024,[63,138,147,193,248,318,514,522,583,652,770,786,864],[4.451660326432682,2.273418200008753,5.070699534838906,6.193237017633792,5.367951058306837,9.311511365552395,3.520102122427739,5.330210730323991,1.835950360814415,4.6557556827761974,4.60069590559317,7.2576706451284085,5.04252865787221])                                                                                                                                                   |\n",
      "|[gonna, home, soon, want, talk, stuff, anymore, tonight, k, cried, enough, today]                                                            |(1024,[85,125,314,364,428,445,485,721,750,776,816,942],[3.698793911171115,3.981656697186947,3.898659777615979,3.9255672305359033,5.535005142970004,4.451660326432682,3.1579874551931884,3.907548725033225,3.027625637329945,3.4555636012901676,2.6938024088803085,5.04252865787221])                                                                                                                                                                      |\n",
      "|[six, chances, win, cash, pounds, txt>, csh, send, cost, p/day, days, +, tsandcs, apply, reply, hl, info]                                    |(1024,[31,62,122,163,204,247,362,445,457,465,484,633,659,711,803,851,880],[3.6632029660684124,4.072170704727778,3.496148881405246,3.255409568200657,5.917997395226109,4.148710781850113,3.8638736615305636,4.451660326432682,3.6772877059501514,4.637063549764045,4.912475529624012,4.160139477673736,4.498913211283228,6.141140946540319,3.934699714099176,3.44989786375449,4.797406199839225])                                                          |\n",
      "|[urgent, won, week, free, membership, prize, jackpot, txt, word, claim, t&c, www, dbuk, net, lccltd, pobox, ldnw, rw]                        |(1024,[193,278,338,378,389,429,452,479,537,560,573,681,695,714,773,808,956,1001],[3.096618508816896,4.797406199839225,3.822026551595063,5.986990266713061,3.532297395521557,3.342843867590331,4.001074783044048,4.797406199839225,3.5698017909800117,5.258751766341845,4.256599743861298,3.7207728178898902,4.937168142214383,4.618714411095849,3.934699714099176,4.295314256041989,5.367951058306837,4.5656045857819])                                   |\n",
      "|[searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, wonderful, blessing, times]                  |(1024,[29,181,225,237,243,312,343,399,463,609,756,814,887,929],[6.141140946540319,4.819385106557999,4.864847480634757,4.451660326432682,3.7740173324087025,4.7758999946182605,3.602167075482043,10.587686172306231,5.447993765980374,5.407171771460119,5.293843086153116,3.907548725033225,4.864847480634757,4.6557556827761974])                                                                                                                         |\n",
      "|[date, sunday]                                                                                                                               |(1024,[360,592],[4.7758999946182605,5.49055338039917])                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|[xxxmobilemovieclub, use, credit, click, wap, link, next, txt, message, click, here>>, http, //wap, xxxmobilemovieclub, com, n=qjkgighjjgcbl]|(1024,[75,166,207,281,329,413,429,522,598,615,635,689,764,1023],[3.813863240955902,10.199374143424317,5.258751766341845,6.228152323529949,4.58299632849377,4.282242174474636,3.342843867590331,5.330210730323991,4.030927746193729,4.207206988531722,11.363217234323757,4.061699404860483,4.819385106557999,4.269338769638728])                                                                                                                           |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('Limpando o texto - remover pontuação, números e espaços adicionais')\n",
    "#colinha: regex para remover específicas pontuações e números [_():;,.!?\\\\-0-9]\n",
    "\n",
    "sms_df = sms_df.withColumn('text', F.regexp_replace(sms_df.text, '[_():;,.!?\\\\-0-9]', ' '))\n",
    "sms_df = sms_df.withColumn('text', F.regexp_replace(sms_df.text, ' +', ' '))\n",
    "\n",
    "print('Tokenizando e removendo stop words')\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "sms_tk = Tokenizer(inputCol='text', outputCol='tokens').transform(sms_df)\n",
    "sms_tkcl = StopWordsRemover(inputCol='tokens', outputCol='tokens_clean').transform(sms_tk)\n",
    "\n",
    "print('Calculando TF')\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "sms_tf = HashingTF(inputCol='tokens_clean', outputCol='tf', numFeatures=1024).transform(sms_tkcl)\n",
    "\n",
    "print('Calculando TF-IDF')\n",
    "from pyspark.ml.feature import IDF\n",
    "sms_tfidf = IDF(inputCol='tf', outputCol='features').fit(sms_tf).transform(sms_tf)\n",
    "sms_tfidf.select('tokens_clean', 'features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788ad39",
   "metadata": {},
   "source": [
    "## Algoritmos de aprendizado de máquina em Spark\n",
    "\n",
    "Veremos quatro categorias de algoritmos de ML a seguir:\n",
    "- Classificação\n",
    "- Regressão\n",
    "- Agrupamento\n",
    "- Recomendação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d25776",
   "metadata": {},
   "source": [
    "### Classificação\n",
    "\n",
    "Os algoritmos de classificação aprendem um modelo que é capaz de discernir instâncias entre múltiplas classes. Os algoritmos dessa categoria estão localizados no módulo `pyspark.ml.classification`. Podemos olhar a [documentação](https://spark.apache.org/docs/latest/ml-classification-regression.html#classification) para verificar quais algoritmos estão disponíveis. No caso desse notebook, olharemos para dois algoritmos clássicos:\n",
    "\n",
    "- Árvore de Decisão\n",
    "- Regressão Logística\n",
    "\n",
    "Para avaliar a performance dos algoritmos, podemos utilizar dois objetos:`MulticlassClassificationEvaluator` e `BinaryClassificationEvaluator`. O primeiro lida com avaliação de modelos capazes discretizar entre múltiplas classes e contém métricas como precisão, revocação, medida-F por classe e/ou ponderada. Já o segundo, foca em análise de classificação binária e possui implementações específicas para esse caso, como AUC. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e19adf",
   "metadata": {},
   "source": [
    "#### Árvore de Decisão\n",
    "\n",
    "Árvore de decisão é um dos algoritmos mais clássicos de aprendizado de máquina. O algoritmo escolhe a característica mais importante para particionar o espaço, o divide e recursivamente o invoca para resolver os subconjuntos resultantes.\n",
    "\n",
    "![Arvore de decisão para jogar tênis, retirado de: https://www.researchgate.net/figure/Decision-tree-for-conditions-to-play-tennis_fig1_283569105](https://www.researchgate.net/profile/Peter-Wagacha/publication/283569105/figure/fig1/AS:293585607639040@1447007671951/Decision-tree-for-conditions-to-play-tennis.png)\n",
    "\n",
    "\n",
    "Vamos rodar `DecisionTreeClassifier` para aprender um modelo com `flights_vec` e com `sms_tfidf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3df3cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodando o modelo e vendo o resultado\n",
      "+-----+----------+-----------------------------------------+\n",
      "|Y_bin|prediction|probability                              |\n",
      "+-----+----------+-----------------------------------------+\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|0    |0.0       |[0.8686296715741789,0.13137032842582105] |\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|0    |0.0       |[0.8686296715741789,0.13137032842582105] |\n",
      "|0    |0.0       |[0.8686296715741789,0.13137032842582105] |\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|1    |0.0       |[0.8686296715741789,0.13137032842582105] |\n",
      "|1    |0.0       |[0.8686296715741789,0.13137032842582105] |\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|0    |0.0       |[0.8686296715741789,0.13137032842582105] |\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|0    |0.0       |[0.5843373493975904,0.41566265060240964] |\n",
      "|1    |1.0       |[0.005563282336578581,0.9944367176634215]|\n",
      "|0    |0.0       |[0.9073053892215569,0.09269461077844311] |\n",
      "|1    |1.0       |[0.005563282336578581,0.9944367176634215]|\n",
      "|0    |0.0       |[0.5843373493975904,0.41566265060240964] |\n",
      "+-----+----------+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Matriz de confusão\n",
      "+-----+----------+-----+\n",
      "|Y_bin|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  265|\n",
      "|    0|       0.0| 1430|\n",
      "|    1|       1.0|  271|\n",
      "|    0|       1.0|   51|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "#dividir em treino/teste\n",
    "flights_train, flights_test = flights_vec.randomSplit([0.8,0.2], seed=123)\n",
    "\n",
    "print('Rodando o modelo e vendo o resultado')\n",
    "dt_flights_model = DecisionTreeClassifier(featuresCol='features', labelCol='Y_bin')\n",
    "preds_dt_flight = dt_flights_model.fit(flights_train).transform(flights_test)\n",
    "preds_dt_flight.select('Y_bin', 'prediction', 'probability').show(truncate=False)\n",
    "\n",
    "print('Matriz de confusão')\n",
    "preds_dt_flight.groupBy('Y_bin', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d472f",
   "metadata": {},
   "source": [
    "Vamos avaliar o modelo? Vamos ver sua eficácia em termos de precisão e revocação ponderadas, e AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca369fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8431149316810882\n",
      "Recall: 0.8433316807139315\n",
      "AUC: 0.5252230995595957\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "multi_eval = MulticlassClassificationEvaluator().setLabelCol('Y_bin')\n",
    "prec_dt_flight = multi_eval.evaluate(preds_dt_flight, {multi_eval.metricName: 'weightedPrecision'})\n",
    "recall_dt_flight = multi_eval.evaluate(preds_dt_flight, {multi_eval.metricName: 'weightedRecall'})\n",
    "\n",
    "bin_eval = BinaryClassificationEvaluator().setLabelCol('Y_bin')\n",
    "AUC_dt_flight = bin_eval.evaluate(preds_dt_flight, {bin_eval.metricName:'areaUnderROC'})\n",
    "\n",
    "print(f'Precision: {prec_dt_flight}')\n",
    "print(f'Recall: {recall_dt_flight}')\n",
    "print(f'AUC: {AUC_dt_flight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "372928fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 279:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|  Y|prediction|count|\n",
      "+---+----------+-----+\n",
      "|  1|       0.0|   78|\n",
      "|  0|       0.0|  962|\n",
      "|  1|       1.0|   65|\n",
      "|  0|       1.0|    8|\n",
      "+---+----------+-----+\n",
      "\n",
      "Precision: 0.9205559453039422\n",
      "Recall: 0.9227313566936208\n",
      "AUC: 0.36334799221397157\n"
     ]
    }
   ],
   "source": [
    "# rodar arvore de decisão com sms_tfidf\n",
    "\n",
    "sms_train, sms_test = sms_tfidf.randomSplit([0.8,0.2], seed=123)\n",
    "\n",
    "preds_dt_sms = DecisionTreeClassifier(featuresCol='features', labelCol='Y').fit(sms_train).transform(sms_test)\n",
    "preds_dt_sms.groupBy('Y', 'prediction').count().show()\n",
    "\n",
    "multi_eval = MulticlassClassificationEvaluator().setLabelCol('Y')\n",
    "prec_dt_sms = multi_eval.evaluate(preds_dt_sms, {multi_eval.metricName: 'weightedPrecision'})\n",
    "recall_dt_sms = multi_eval.evaluate(preds_dt_sms, {multi_eval.metricName: 'weightedRecall'})\n",
    "\n",
    "bin_eval = BinaryClassificationEvaluator().setLabelCol('Y')\n",
    "AUC_dt_sms = bin_eval.evaluate(preds_dt_sms)\n",
    "\n",
    "print(f'Precision: {prec_dt_sms}')\n",
    "print(f'Recall: {recall_dt_sms}')\n",
    "print(f'AUC: {AUC_dt_sms}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa592a1d",
   "metadata": {},
   "source": [
    "#### Regressão Logística\n",
    "\n",
    "Na regressão logística, utiliza-se uma função sigmoide para realizar classificação binária:  \n",
    "\n",
    "![Disponível em: https://www.javatpoint.com/logistic-regression-in-machine-learning](https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning.png)\n",
    "\n",
    "Vamos rodar `flights_vec` e `sms_tfidf` também nesse modelo e comparar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a224e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Y_bin|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  244|\n",
      "|    0|       0.0| 1417|\n",
      "|    1|       1.0|  292|\n",
      "|    0|       1.0|   64|\n",
      "+-----+----------+-----+\n",
      "\n",
      "Precision: 0.8443640812625656\n",
      "Recall: 0.8472979672781359\n",
      "AUC: 0.8342852751771241\n"
     ]
    }
   ],
   "source": [
    "# rodar regressão logística com flights_vec\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logrec_model = LogisticRegression(featuresCol='features', labelCol='Y_bin')\n",
    "preds_log_flights = logrec_model.fit(flights_train).transform(flights_test)\n",
    "preds_log_flights.groupBy('Y_bin', 'prediction').count().show()\n",
    "\n",
    "multi_eval = MulticlassClassificationEvaluator().setLabelCol('Y_bin')\n",
    "prec_log_flight = multi_eval.evaluate(preds_log_flights, {multi_eval.metricName: 'weightedPrecision'})\n",
    "recall_log_flight = multi_eval.evaluate(preds_log_flights, {multi_eval.metricName: 'weightedRecall'})\n",
    "\n",
    "bin_eval = BinaryClassificationEvaluator().setLabelCol('Y_bin')\n",
    "AUC_log_flight = bin_eval.evaluate(preds_log_flights, {bin_eval.metricName:'areaUnderROC'})\n",
    "\n",
    "print(f'Precision: {prec_log_flight}')\n",
    "print(f'Recall: {recall_log_flight}')\n",
    "print(f'AUC: {AUC_log_flight}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91cc5efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|  Y|prediction|count|\n",
      "+---+----------+-----+\n",
      "|  1|       0.0|   15|\n",
      "|  0|       0.0|  929|\n",
      "|  1|       1.0|  128|\n",
      "|  0|       1.0|   41|\n",
      "+---+----------+-----+\n",
      "\n",
      "Precision: 0.954981632268169\n",
      "Recall: 0.949685534591195\n",
      "AUC: 0.9598947444308443\n"
     ]
    }
   ],
   "source": [
    "# rodar regressão logística com sms_tfidf\n",
    "preds_log_sms = LogisticRegression(featuresCol='features', labelCol='Y').fit(sms_train).transform(sms_test)\n",
    "preds_log_sms.groupBy('Y', 'prediction').count().show()\n",
    "\n",
    "multi_eval = MulticlassClassificationEvaluator().setLabelCol('Y')\n",
    "prec_log_sms = multi_eval.evaluate(preds_log_sms, {multi_eval.metricName: 'weightedPrecision'})\n",
    "recall_log_sms = multi_eval.evaluate(preds_log_sms, {multi_eval.metricName: 'weightedRecall'})\n",
    "\n",
    "bin_eval = BinaryClassificationEvaluator().setLabelCol('Y')\n",
    "AUC_log_sms = bin_eval.evaluate(preds_log_sms)\n",
    "\n",
    "print(f'Precision: {prec_log_sms}')\n",
    "print(f'Recall: {recall_log_sms}')\n",
    "print(f'AUC: {AUC_log_sms}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91376c",
   "metadata": {},
   "source": [
    "### Regressão\n",
    "\n",
    "Em regressão o modelo é treinado para prever um valor contínuo ao invés de uma classe. Os algoritmos dessa categoria estão localizados no módulo `pyspark.ml.regression`. Podemos olhar a [documentação](https://spark.apache.org/docs/latest/ml-classification-regression.html#regression) para verificar quais algoritmos estão disponíveis. No caso desse notebook, olharemos para os seguintes algoritmos:\n",
    "\n",
    "- Regressão Linear\n",
    "- Random Forest para regressão\n",
    "\n",
    "Para avaliar a performance dos algoritmos, podemos utilizar `RegressionEvaluator`. Nele há a implementação de métricas de erro, como MSE, RMSE, R2, MAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e37a48",
   "metadata": {},
   "source": [
    "#### Regressão Linear\n",
    "\n",
    "Regressão linear é o algoritmo mais simples de aprendizado de máquina. Ele tenta encontrar um modelo que descreva os dados a partir de uma relação **linear**. \n",
    "\n",
    "![Disponivel em: https://www.researchgate.net/figure/Linear-Regression-model-sample-illustration_fig3_333457161](https://www.researchgate.net/profile/Hieu-Tran-17/publication/333457161/figure/fig3/AS:763959762247682@1559153609649/Linear-Regression-model-sample-illustration.ppm)\n",
    "\n",
    "Para algoritmos de regressão, utilizaremos como alvo a coluna `'Y_num'` de `flights_vec`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96324853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "print('Treinar e exibir resultados')\n",
    "\n",
    "print('Avaliar')\n",
    "\n",
    "print('Treinar e exibir resultados com regularização')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704649cc",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "O algoritmo de Random Forests é um algoritmo de _ensemble_, ou seja, ele ajusta vários modelos que decidem a classe por meio de votação.\n",
    "\n",
    "![Disponível em: https://en.wikipedia.org/wiki/Random_forest](https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png)\n",
    "\n",
    "Vamos executá-lo e comparar com Regressão Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dc1b46c",
   "metadata": {},
   "source": [
    "### Recomendação\n",
    "\n",
    "ALgoritmos de recomendação capturam as interações dos usuário e produzem sugestões com base nelas. Em Spark apenas um único algoritmo está disponível dentro do módulo `pyspark.ml.recommendation` ([documentação](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html)). Esse algoritmo é o chamado _Alternating Least Squares_ (`ALS`), uma forma de fatoração de matrizes.\n",
    "\n",
    "![Disponível em: https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-2-alternating-least-square-als-matrix-4a76c58714a1](https://miro.medium.com/max/1838/1*xMxQL_V9CWeLggrk-Uyzmg.png)\n",
    "\n",
    "Essa implementação suporta respostas explícitas e implícitas, e é robusta a _cold start_. Vamos fazer um exemplo usando o dataset [HetRec 2011 - MovieLens](https://grouplens.org/datasets/hetrec-2011/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49186bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lendo dataset\n",
    "\n",
    "#dividindo dataset\n",
    "\n",
    "#rodando modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3da47",
   "metadata": {},
   "source": [
    "### Agrupamento\n",
    "\n",
    "Algoritmos de agrupamento são utilizados quando queremos encontrar/aprender padrões em conjuntos de dados. Essa classe de algoritmos está no módulo `pyspark.ml.clustering` (veja a documentação [aqui](https://spark.apache.org/docs/latest/ml-pipeline.html)).\n",
    "\n",
    "Como exemplo veremos o clássico `KMeans`, onde tenta-se particionar o conjunto de dados ao encontrar centróides que representam os dados.\n",
    "\n",
    "![Disponivel em: https://medium.com/@luigi.fiori.lf0303/k-means-clustering-using-python-db57415d26e6](https://miro.medium.com/max/1200/1*TmvsQ4XaOxeb-TmKk1qgOw.png)\n",
    "\n",
    "\n",
    "Podemos avaliar clusters com o objeto `ClusteringEvaluator`, que possui implementação da métrica de silhueta. Porém, agrupamento costumeiramente é melhor visualizado através de gráficos. Ao invés de avaliarmos a qualidade dos grupos por uma métrica, iremos fazer plotando os pontos e os centróides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0bf6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ler points.csv\n",
    "\n",
    "#visualizar dataset\n",
    "\n",
    "#rodar kmeans\n",
    "\n",
    "#visualizar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fff895",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Até agora executamos cada passo separadamente, desde a preparação dos dados até a execução do algoritmo de ML. Spark oferece o recurso `Pipeline`, que permite unir todos os processos de transformação de características e o algoritmo em uma única execução. Você pode ver a documentação sobre pipelines [aqui](https://spark.apache.org/docs/latest/ml-pipeline.html). \n",
    "\n",
    "Pipelines são importantes pelo simples fato que auxiliam a evitar o problema de _vazamento de dados (data leakage)_, garantindo que todo o processo seja executado na partição de treinamento primeiro, e depois na partição de teste.\n",
    "\n",
    "Vamos fazer o Pipeline para `flights_df`, encerrando com uma regressão linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82768e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divindo novamente a base\n",
    "\n",
    "#construindo o pipeline\n",
    "\n",
    "#checando os passos\n",
    "\n",
    "#rodando o pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148556f",
   "metadata": {},
   "source": [
    "Vamos fazer o pipeline para `sms_df`, terminando em uma regressão logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66ca12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09590271",
   "metadata": {},
   "source": [
    "## Otimização e Validação Cruzada\n",
    "\n",
    "Spark oferece também ferramentas de otimização e seleção de hiperparâmetros para ML, dentro do módulo `pyspark.ml.tuning` (veja a \n",
    "[documentação](https://spark.apache.org/docs/latest/ml-tuning.html)). Muitos algoritmos possuem vários hiperparâmetros que muitas das vezes só conseguem ser definidos empiricamente, rodando diversas configurações ou empregando técnicas de _busca em grade_. \n",
    "\n",
    "Para busca em grade, Spark oferece o objeto `ParamGridBuilder`. Esse recurso deve ser empregado em conjunto com um mecanismo de _Validação Cruzada_, executado com um `CrossValidator`. A Validação Cruzada é uma maneira muito eficiente de fazer otimização de parâmetros, pois é executada em diversas repartições dos dados de treinamento e oferece diversas visões de como um modelo se comporta. \n",
    "\n",
    "![Disponível em: https://drigols.medium.com/introdu%C3%A7%C3%A3o-a-valida%C3%A7%C3%A3o-cruzada-k-fold-2a6bced32a90](https://miro.medium.com/max/1202/0*O_491U1UfF1lIqz_.png)\n",
    "\n",
    "Os módulos de otimização podem ser utilizados em conjunto com Pipeline. Vamos aproveitar os pipelines criados anteriormente e tentar otimizar os parâmetros dos algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# otimização para pipeline_flights\n",
    "\n",
    "#criando um grid\n",
    "\n",
    "#criando um avaliador\n",
    "\n",
    "#rodando validação cruzada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb4b54",
   "metadata": {},
   "source": [
    "Vamos ver os parâmetros escolhidos para o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd96b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2eedf25",
   "metadata": {},
   "source": [
    "E finalmente, avaliar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c98e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f813d3",
   "metadata": {},
   "source": [
    "Para finalizar, vamos executar um cross validation no nosso `sms_df`. Aproveite o pipeline construído. Uma vantagem do `ParamGridBuilder` é que ele não se limita a ajustar parâmetros do algoritmo de ML. Tente ajustar o parâmetro de `HashingTF` também, além dos parâmetros selecionados para regressão logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3120c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96eaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a237b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f3424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
