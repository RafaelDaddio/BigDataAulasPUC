{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a278e3",
   "metadata": {},
   "source": [
    "# Dataframes e SparkSQL\n",
    "\n",
    "Spark oferece abstrações de mais alto nível chamadas APIs estruturadas. Ao todo, são três: Dataframes, Datasets e SparkSQL. \n",
    "\n",
    "![Pilha de APIs, disponível em: https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/](https://izhangzhihao.github.io/assets/images/spark-05.png)\n",
    "\n",
    "Datasets não são implementados em PySpark, porém veremos nesse notebook exemplos de uso das outras duas.\n",
    "\n",
    "**Dataframes** são estruturas tabulares, assim como os DataFrames de Pandas. A diferença, aqui, é que Dataframes em Spark são _distribuídos_ e construídos em cima de RDDs. \n",
    "\n",
    "**SparkSQL** é um conjunto de funcionalidades que são operadas em Dataframes. Veremos que podemos manipular Dataframes tanto programaticamente (através de _transformações_) quanto por linguagem SQL. Além disso, SparkSQL oferece uma série de outras ferramentas para a realização de operações tabulares distribuídas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518f377",
   "metadata": {},
   "source": [
    "Vamos começar importando PySpark e criando um objeto `SparkContext`, assim como fizemos anteriormente. Lembre-se que `SparkContext` é a porta de entrada para manipulação de RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30c42e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 09:45:33,761 WARN util.Utils: Your hostname, bigdatavm-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "2021-12-11 09:45:33,763 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-12-11 09:45:34,923 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName='Dataframes e SparkSQL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b490bc8",
   "metadata": {},
   "source": [
    "# O objeto SparkSession\n",
    "\n",
    "Para manipularmos Dataframes e utilizar as funções do SparkSQL, porém, precisamos criar um objeto `SparkSession`. O `SparkSession` é um objeto que é construído em cima do SparkContext, e é o ponto de entrada para as APIs estruturadas do Spark. \n",
    "https://imgs.developpaper.com/imgs/147289973-592685e12263f_articlex.png ![SparkSession vs SparkContext em: https://developpaper.com/application-sparksession-sparkcontext-and-rdd-in-spark-and-their-extensions/](https://imgs.developpaper.com/imgs/147289973-592685e12263f_articlex.png)\n",
    "\n",
    "Para inicializar um SparkSession, importamos sua definição de `pyspark.sql` e criamos um objeto com `SparkSession.builder.getOrCreate()`. Esse método checa se já existe um `SparkSession` ativo, e se não existir, cria um novo. Observe que não precisei mencionar o `SparkContext`. Ele recupera sua instância automaticamente, ou a cria se não existir também."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9dc6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fc654950e20>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83750b7c",
   "metadata": {},
   "source": [
    "# Criando Dataframes\n",
    "\n",
    "Dataframes são objetos tabulares (lembre-se do _Pandas_) distribuídos. Você pode criar um dataframe lendo arquivos csv, JSON, [ORC](https://orc.apache.org/) e [Parquet](https://github.com/apache/parquet-format). Além disso, você pode criar Dataframes a partir de RDDs também. \n",
    "\n",
    "Vamos fazer alguns exemplos com a versão simplificada de leitura e criação de arquivos. Para mais opções, consultar a [documentação](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c349da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+---+---+----+------+----+------+------+-------+\n",
      "|       _c0|  _c1|_c2|_c3|_c4| _c5|   _c6| _c7|   _c8|   _c9|   _c10|\n",
      "+----------+-----+---+---+---+----+------+----+------+------+-------+\n",
      "|2014-04-01|19805|  1|JFK|LAX|0854| -6.00|1217|  2.00|355.00|2475.00|\n",
      "|2014-04-01|19805|  2|LAX|JFK|0944| 14.00|1736|-29.00|269.00|2475.00|\n",
      "|2014-04-01|19805|  3|JFK|LAX|1224| -6.00|1614| 39.00|371.00|2475.00|\n",
      "|2014-04-01|19805|  4|LAX|JFK|1240| 25.00|2028|-27.00|264.00|2475.00|\n",
      "|2014-04-01|19805|  5|DFW|HNL|1300| -5.00|1650| 15.00|510.00|3784.00|\n",
      "|2014-04-01|19805|  6|OGG|DFW|1901|126.00|0640| 95.00|385.00|3711.00|\n",
      "|2014-04-01|19805|  7|DFW|OGG|1410|125.00|1743|138.00|497.00|3711.00|\n",
      "|2014-04-01|19805|  8|HNL|DFW|1659|  4.00|0458|-22.00|398.00|3784.00|\n",
      "|2014-04-01|19805|  9|JFK|LAX|0648| -7.00|1029| 19.00|365.00|2475.00|\n",
      "|2014-04-01|19805| 10|LAX|JFK|2156| 21.00|0556|  1.00|265.00|2475.00|\n",
      "+----------+-----+---+---+---+----+------+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importando de um arquivo csv\n",
    "\n",
    "flights_df = spark.read.csv('file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/Flights_old/flights.csv')\n",
    "flights_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd62a45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hamlet@0\\t\\tHAMLET', 'hamlet@8\\t', 'hamlet@9\\t', 'hamlet@10\\t\\tDRAMATIS PERSONAE', 'hamlet@29\\t']\n",
      "[('hamlet@0', 'HAMLET'), ('hamlet@8', ''), ('hamlet@9', ''), ('hamlet@10', 'DRAMATIS PERSONAE'), ('hamlet@29', '')]\n",
      "+---------+-----------------+\n",
      "|       _1|               _2|\n",
      "+---------+-----------------+\n",
      "| hamlet@0|           HAMLET|\n",
      "| hamlet@8|                 |\n",
      "| hamlet@9|                 |\n",
      "|hamlet@10|DRAMATIS PERSONAE|\n",
      "|hamlet@29|                 |\n",
      "+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# criando a partir de um RDD\n",
    "\n",
    "text_rdd = sc.textFile('file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/shakespeare.txt')\n",
    "print(text_rdd.take(5))\n",
    "\n",
    "def extract_key(line):\n",
    "    split = line.split('\\t', 1)\n",
    "    return (split[0], split[1].replace('\\t', ' ').strip())\n",
    "\n",
    "text_rdd = text_rdd.map(extract_key)\n",
    "print(text_rdd.take(5))\n",
    "\n",
    "text_df = spark.createDataFrame(text_rdd)\n",
    "text_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e3986",
   "metadata": {},
   "source": [
    "# Schemas\n",
    "\n",
    "Quando lendo de um csv ou criando um Dataframe a partir de um RDD, o Spark pode fazer múltiplas leituras para automaticamente tentar inferir qual é o _Schema_ do Dataframe, ou seja, qual é a tipagem de cada coluna e se ela pode ter ou não valores _null_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc41630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      "\n",
      "0.722050666809082\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "flights_df.printSchema()\n",
    "\n",
    "#checar schema\n",
    "start_time = time.time()\n",
    "flights_df = spark.read.csv('file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/Flights_old/flights.csv', inferSchema=True)\n",
    "print(time.time() - start_time)\n",
    "flights_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd40fa",
   "metadata": {},
   "source": [
    "A inferência de _Schema_ pode causar lentidão na leitura dos dados, devido a necessidade de eles serem lidos múltiplas vezes. Você pode, alternativamente, definir um _Schema_ e utilizá-lo na leitura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3188edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020491600036621094\n"
     ]
    }
   ],
   "source": [
    "#criar um Schema e reimportar flights.csv\n",
    "\n",
    "import pyspark.sql.types as types\n",
    "\n",
    "flights_headers_file = '/home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/Flights_old/flights_header.csv'\n",
    "with open(flights_headers_file) as f:\n",
    "    flights_header = f.read()\n",
    "\n",
    "list_headers = flights_header.split(',')\n",
    "\n",
    "flight_schema = types.StructType() \\\n",
    "                        .add(list_headers[0], types.DateType(), True) \\\n",
    "                        .add(list_headers[1], types.IntegerType(), True) \\\n",
    "                        .add(list_headers[2], types.IntegerType(), True) \\\n",
    "                        .add(list_headers[3], types.StringType(), True) \\\n",
    "                        .add(list_headers[4], types.StringType(), True) \\\n",
    "                        .add(list_headers[5], types.IntegerType(), True) \\\n",
    "                        .add(list_headers[6], types.DoubleType(), True) \\\n",
    "                        .add(list_headers[7], types.IntegerType(), True) \\\n",
    "                        .add(list_headers[8], types.DoubleType(), True) \\\n",
    "                        .add(list_headers[9], types.DoubleType(), True) \\\n",
    "                        .add(list_headers[10].strip(), types.DoubleType(), True) \\\n",
    "\n",
    "start_time = time.time()\n",
    "flight_schema_df = spark.read.format('csv').schema(flight_schema).load('file:///home/bigdata-vm/Desktop/BigDataAulasPUC/Datasets/Flights_old/flights.csv')\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9c087",
   "metadata": {},
   "source": [
    "# Operações em Dataframes\n",
    "\n",
    "Vamos começar a realizar operações em Dataframes. Nesse primeiro momento, iremos ver programaticamente algumas das principais funções para manipulação de Dataframes. A maioria aqui é _transformação_, exceto aquelas que precisam devolver algum retorno para o Programa _Driver_, como `count()`, `show()` e `collect()`.\n",
    "\n",
    "### Visualizar informações sobre um Dataframe\n",
    "\n",
    "Vamos ver algumas funções básicas para retornar informações a respeito do Dataframe como um todo. São elas:\n",
    "- `count()`: retorna o número de linhas\n",
    "- `len(df.columns)`: `df.columns` retorna uma lista com o nome das colunas. Se aplicarmos `len()`, podemos ver a quantidade de colunas em um Dataframe.\n",
    "- `describe()`: retorna estatísticas sobre as colunas de um Dataframe. Podemos definir quais colunas queremos, ou a tabela inteira se não passarmos parâmetros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09769f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas 476881\n",
      "Quantidade de colunas 11\n",
      "Estatisticas gerais\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|     id_companhia|         num_voos|origem|destino|      hora_partida|min_atraso_partida|      hora_chegada|min_atraso_chegada|           duracao|        distancia|\n",
      "+-------+-----------------+-----------------+------+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|           476881|           476881|476881| 476881|            476881|            476881|            476881|            476881|            476881|           476881|\n",
      "|   mean|19990.46749818089| 2245.92602766728|  null|   null|1334.1795374527399| 8.313877046894298| 1485.241848595352| 4.728577989058067|111.04921353545224|794.8585013871385|\n",
      "| stddev|398.4945595986355|1841.066184447977|  null|   null|483.03628051072656| 33.34531644580256|509.40814513813245|35.507045105136605| 71.54639383230081|596.0053280615442|\n",
      "|    min|            19393|                1|   ABE|    ABE|                 0|             -55.0|                 0|             -88.0|               8.0|             31.0|\n",
      "|    max|            21171|             8406|   YUM|    YUM|              2359|            1696.0|              2359|            1793.0|             664.0|           4983.0|\n",
      "+-------+-----------------+-----------------+------+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|min_atraso_partida|min_atraso_chegada|\n",
      "+-------+------------------+------------------+\n",
      "|  count|            476881|            476881|\n",
      "|   mean| 8.313877046894298| 4.728577989058067|\n",
      "| stddev| 33.34531644580256|35.507045105136605|\n",
      "|    min|             -55.0|             -88.0|\n",
      "|    max|            1696.0|            1793.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Quantidade de linhas {flight_schema_df.count()}')\n",
    "print(f'Quantidade de colunas {len(flight_schema_df.columns)}')\n",
    "print('Estatisticas gerais')\n",
    "flight_schema_df.describe().show()\n",
    "flight_schema_df.describe(['min_atraso_partida', 'min_atraso_chegada']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e49273c",
   "metadata": {},
   "source": [
    "### Select\n",
    "\n",
    "O comando `select()` opera em nível de **coluna**, filtrando quais colunas você quer exibir. Ele pode ainda criar novas colunas, com métodos que analisam valores de uma ou mais colunas linha a linha. É o equivalente ao comando `SELECT` de uma consulta SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5059a8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecionando id da companhia, origem e destino\n",
      "+------------+------+-------+\n",
      "|id_companhia|origem|destino|\n",
      "+------------+------+-------+\n",
      "|       19805|   JFK|    LAX|\n",
      "|       19805|   LAX|    JFK|\n",
      "|       19805|   JFK|    LAX|\n",
      "|       19805|   LAX|    JFK|\n",
      "|       19805|   DFW|    HNL|\n",
      "|       19805|   OGG|    DFW|\n",
      "|       19805|   DFW|    OGG|\n",
      "|       19805|   HNL|    DFW|\n",
      "|       19805|   JFK|    LAX|\n",
      "|       19805|   LAX|    JFK|\n",
      "|       19805|   LAX|    JFK|\n",
      "|       19805|   OGG|    LAX|\n",
      "|       19805|   BOS|    ORD|\n",
      "|       19805|   SFO|    JFK|\n",
      "|       19805|   ATL|    MIA|\n",
      "|       19805|   SFO|    JFK|\n",
      "|       19805|   JFK|    LAX|\n",
      "|       19805|   SFO|    JFK|\n",
      "|       19805|   JFK|    LAX|\n",
      "|       19805|   LAX|    JFK|\n",
      "+------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Selecionando id da companhia, origem e destino e colocando o tempo total de atraso do vôo\n",
      "+------------+------+-------+------------+\n",
      "|id_companhia|origem|destino|atraso_total|\n",
      "+------------+------+-------+------------+\n",
      "|       19805|   JFK|    LAX|        -4.0|\n",
      "|       19805|   LAX|    JFK|       -15.0|\n",
      "|       19805|   JFK|    LAX|        33.0|\n",
      "|       19805|   LAX|    JFK|        -2.0|\n",
      "|       19805|   DFW|    HNL|        10.0|\n",
      "|       19805|   OGG|    DFW|       221.0|\n",
      "|       19805|   DFW|    OGG|       263.0|\n",
      "|       19805|   HNL|    DFW|       -18.0|\n",
      "|       19805|   JFK|    LAX|        12.0|\n",
      "|       19805|   LAX|    JFK|        22.0|\n",
      "|       19805|   LAX|    JFK|       -42.0|\n",
      "|       19805|   OGG|    LAX|       -12.0|\n",
      "|       19805|   BOS|    ORD|       -28.0|\n",
      "|       19805|   SFO|    JFK|       -16.0|\n",
      "|       19805|   ATL|    MIA|       -22.0|\n",
      "|       19805|   SFO|    JFK|       200.0|\n",
      "|       19805|   JFK|    LAX|        12.0|\n",
      "|       19805|   SFO|    JFK|       255.0|\n",
      "|       19805|   JFK|    LAX|        12.0|\n",
      "|       19805|   LAX|    JFK|         9.0|\n",
      "+------------+------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Selecionando data, origem, destino e o tempo de voo em horas\n"
     ]
    }
   ],
   "source": [
    "print('Selecionando id da companhia, origem e destino')\n",
    "flight_schema_df.select(flight_schema_df.id_companhia, flight_schema_df.origem, flight_schema_df.destino).show()\n",
    "print('Selecionando id da companhia, origem e destino e colocando o tempo total de atraso do vôo')\n",
    "flight_schema_df.select('id_companhia', 'origem', 'destino', (flight_schema_df.min_atraso_partida + flight_schema_df.min_atraso_chegada).alias('atraso_total')).show()\n",
    "print('Selecionando data, origem, destino e o tempo de voo em horas')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb1a39",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "A função `filter()` opera em nível de **linhas**, e utiliza operações lógicas que retornam resultados _binários_. É similar ao comando `WHERE` de uma consulta SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Apresentar apenas os vôos com saída adiantada')\n",
    "print('Apresentar todos os vôos que não saíram nem chegaram em JFK')\n",
    "print('Selecionar as colunas dataVoo, origem, destino e exibir apenas os vôos com menos de 4 horas de duração')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fab1c",
   "metadata": {},
   "source": [
    "### GroupBy\n",
    "\n",
    "A função `groupBy()` permite realizar operações de agregação no conjunto de dados. Ele equivale ao comando `GROUP BY` de consultas SQL. Pode receber como parâmetro colunas (onde opera a agregação). Se não receber nenhum parâmetro, realiza a agregação na _tabela inteira_. `groupBy()` retorna um objeto `GroupedData`, que contém várias funções de agregação como `sum()`, `mean()`, `max()`, `min()`, entre outros. A função `agg()` permite que seja feita múltiplas agregações (por exemplo, média e desvio padrão) nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Ver o máximo de cada coluna numérica')\n",
    "print('Ver o tempo médio de atraso de chegada para cada companhia')\n",
    "print('Ver o tempo médio e desvio padrão de duracao para cada vôo específico')\n",
    "print('Ver apenas o vôo com menor atraso total (partida + chegada) de cada par origem-destino')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c45dfc",
   "metadata": {},
   "source": [
    "## Adicionar e remover colunas/linhas\n",
    "\n",
    "Mais algumas funções que manipulam linhas e colunas, porém não criam _subconjuntos_ dos dados como as funções acima.\n",
    "\n",
    "- `withColumn()`: adiciona uma coluna nova ao Dataframe e o retorna\n",
    "- `drop()`: remove uma coluna do Dataframe e o retorna\n",
    "- `distinct()`: remove linhas duplicadas  \n",
    "- `dropDuplicates()`: remove linhas duplicadas, porém é possível passar um subconjunto de colunas para realizar a checagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adicionando uma nova coluna com a duração da da viagem em horas')\n",
    "print('Removendo data, hora_partida e hora_chegada e duracao')\n",
    "print('Removendo duplicatas')\n",
    "print('Mantendo apenas um vôo por origem-destino')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2c88d",
   "metadata": {},
   "source": [
    "### OrderBy\n",
    "\n",
    "A função `orderBy()` aceita dois argumentos: uma coluna (ou lista de colunas) que deverão ser usadas na ordenação, e um _boolean_ (ou lista de _booleans_) definindo ordem ascendente ou não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27977434",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ordenando decrescentemente por duração\")\n",
    "print(\"Ordenando ascendentemente por origem e destino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55862d13",
   "metadata": {},
   "source": [
    "### Join\n",
    "\n",
    "A função `join()` une dois Dataframes a partir de uma coluna em comum. Há vários tipos diferentes de joins no Spark:\n",
    "\n",
    "![Tipos de join. Disponível em: https://medium.com/bild-journal/pyspark-joins-explained-9c4fba124839](https://miro.medium.com/max/622/1*6d7MzkjxS0eBWjOJN5TaAQ.jpeg)\n",
    "\n",
    "Todos os tipos de join mostrados acima podem ser usados junto com o método. Vamos ver dois exemplos: `'left_outer'` e `'inner'`. Os demais tipos estão listados na [documentação](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.join.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90788ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Lendo novos datasets')\n",
    "print('Fazendo left outer join')\n",
    "print('Fazendo inner join')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd06322",
   "metadata": {},
   "source": [
    "# Usando SQL\n",
    "\n",
    "Praticamente todas as operações que vimos acima podem ser descritas diretamente em SQL e aplicadas em um Dataframe. Spark possui um método `sql()` que recebe uma string com sintaxe SQL e a processa de maneira distribuída. O resultado é idêntico ao que fizemos programaticamente. No fim, vai de gosto e familiaridade do programador para escolher qual opção prefere.\n",
    "\n",
    "Antes de realizarmos consultas SQL, é importante que criemos uma visualização temporária SQL com `createOrReplaceTempView()`. Vamos fazer alguns exemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2aac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Distancia media de todos voos')\n",
    "print('Tempo medio de cada itinerario')\n",
    "print('Apenas itinerários com tempo medio  menor que 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c884b1",
   "metadata": {},
   "source": [
    "# Usando funções com pyspark.sql.functions\n",
    "\n",
    "O pacote `pyspark.sql.functions` fornece uma série de implementações de funções padrões para a manipulação de Dataframes, Datasets e consultas SQL. Podemos utilizar neste pacote, dentre muitas outras, funções aritméticas e de transformação de `Strings` e `Arrays`. Vamos importar o pacote e usá-lo da seguinte forma:\n",
    "\n",
    "`import pyspark.sql.functions as F`\n",
    "\n",
    "\n",
    "### Transformações de String\n",
    "\n",
    "Vamos começar com algumas funções de manipulação de String:\n",
    "\n",
    "- `upper()` e `lower()`: transformam o texto inteiro em caixa alta ou baixa, respectivamente.\n",
    "\n",
    "- `split()`: quebram um String, transformando ele em uma lista dentro de uma célula de um Dataframe (coluna se torna do tipo `ArrayType()`.\n",
    "\n",
    "- `contains()`: checa se uma célula contém uma substring\n",
    "\n",
    "- `cast()`: converte um String em outro tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Caixa alta e caixa baixa')\n",
    "print('Quebrando texto em tokens')\n",
    "print('Filtrando por KING')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb238b",
   "metadata": {},
   "source": [
    "### Funções com ArrayType()\n",
    "\n",
    "Dataframes conseguem guardar `Arrays` em suas células, mas não conseguem operar diretamente com elas. A manipulação deve ser feita através de funções padrões em `pyspark.sql.functions` ou de UDFs criadas por usuários (veremos elas logo).\n",
    "\n",
    "- `size()`: retorna o tamanho do Array em uma célula.\n",
    "\n",
    "- `getItem()`: retorna um elemento de um Array em uma célula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Descobrindo o numero de elementos em cada linha:')\n",
    "print('Recuperando o primeiro token de cada linha')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a972b1",
   "metadata": {},
   "source": [
    "### Condicionais\n",
    "\n",
    "O pacote `pyspark.sql.functions` fornece também as funções condicionais `when()` e `otherwise()`. \n",
    "\n",
    "- `when()`: Age como uma cláusula `if`.\n",
    "- `otherwise()`: Corresponde ao `else`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Retornar colunas Split, Primeiro_token e criar uma nova verificando se há a palavra \"king\"')\n",
    "print('Criar coluna dizendo se há \"Claudius\" (valor 1), \"Polonius\" (valor 2) ou outra pessoa (valor 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94c566",
   "metadata": {},
   "source": [
    "### Funções definidas por usuários\n",
    "\n",
    "O pacote `pyspark.sql.functions` fornece também a possibilidade de criar funções que ele não tem implementado por padrão. Essas são chamadas de UDFs, ou _User-Defined Functions_, e têm o seguinte formato:\n",
    "\n",
    "`F.udf(funcão, tipo_de_retorno)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19160890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Contar numero de menções \"hamlet\" na linha')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dfaef",
   "metadata": {},
   "source": [
    "# Exercício: modificando `key_text_df`\n",
    "\n",
    "Vamos transformar o nosso Dataframe usado nos exemplos acima. Faça os seguintes passos:\n",
    "- Remova as linhas onde o campo chave esteja em branco \n",
    "- Crie uma UDF para remover tudo que houver antes do @ na coluna chave e salve em uma coluna chamada 'id_linha'\n",
    "- Crie uma coluna com o texto em caixa baixa e a nomeie de 'texto'\n",
    "- Remova as colunas de chave e valor antigas\n",
    "- Crie uma nova coluna com os tokens de 'texto'. A nomeie de 'tokens'\n",
    "- Crie uma coluna com a quantidade de tokens. A nomeie de 'num_tokens'\n",
    "- Remova as linhas em branco e que contém apenas '|'\n",
    "- Extraia o primeiro e o ultimo token de cada linha, e os guarde em colunas nomeadas por 'primeiro' e 'ultimo'\n",
    "- Filtre o dataframe para manter somente linhas que não mencionam 'hamlet' (utilize o sinal ~ como negação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca996cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "951d4406",
   "metadata": {},
   "source": [
    "# Visualizando Dados graficamente\n",
    "\n",
    "Infelizmente, Spark não suporta visualização de dados nativamente. Para visualizá-los, podemos:\n",
    "- Utilizar bibliotecas de terceiros, como a `pyspark_dist_explore` (não recomendado, última versão em 2019), ou\n",
    "- Retirar uma amostra dos dados e transformá-la em Pandas Dataframes, e aproveitar as bibliotecas `Matplotlib` ou `Seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60221fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00677b93",
   "metadata": {},
   "source": [
    "# Particionamento e otimização\n",
    "\n",
    "Veremos algumas funções aqui que lidam diretamente com configurações relacionadas com o formato distribuído dos Dataframes em Spark.\n",
    "\n",
    "\n",
    "### Criando IDs\n",
    "\n",
    "Em um ambiente distribuído, criar IDs seguindo abordagens como contadores e geradores aleatórios não são ideais, dado que múltiplas partições irão lidar com múltiplos _pedaços_ dos dados. Utilizando a função `monotonically_increasing_id()` do pacote `pyspark.sql.functions` possibilita ao Spark criar IDs de forma distribuída e eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84485d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6afb1c0d",
   "metadata": {},
   "source": [
    "### Manipulando partições\n",
    "\n",
    "Muitas das vezes precisamos manipular em quantas partições dividimos nossos dados, para aumentar eficiência. Para manipular partições usamos as seguintes funções:\n",
    "\n",
    "- `rdd.getNumPartitions()`: recupera a quantidade de partições seu RDD ou Dataframe está alocando.\n",
    "\n",
    "- `repartition()`: modifica a quantidade de partições que seu RDD ou Dataframe está alocando.\n",
    "\n",
    "- `coalesce()`: reduz a quantidade de partições (mais eficiente que `repartition()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379de9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "991a78f9",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "_Caching_ é a operação de guardar Dataframes na memória ou em disco. Essa operação pode ajudar a melhorar o desempenho em transformações tardias e reduzir a quantidade de recursos gastos. **Cuidado:** bases de dados muito grandes podem não caber na memória do cluster, e _swap_ com o disco pode ser custoso.\n",
    "\n",
    "Para guardar em cache seu Dataframe, basta invocar a função `cache()`. Para tirá-lo do cache, utilize a função `unpersist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfc421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4d953ed",
   "metadata": {},
   "source": [
    "# Exemplo prático: pipeline de dados\n",
    "\n",
    "Vamos fazer uma análise um pouco mais detalhada de um dataset. Vamos explorar o _\"2017 St Paul MN Real Estate Dataset\"_ e iremos tentar limpar e transformar ele para que seja utilizável por algum algoritmo de aprendizado de máquina para prever o **preço de venda** de um imóvel. Começaremos importando ele e checando seu Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5f999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e42ed3",
   "metadata": {},
   "source": [
    "Olhando o Schema, conseguimos ver que a variável que gostaríamos de prever é a `SalesClosePrice`. Vamos ver sua distribuição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23fd89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9268fbe5",
   "metadata": {},
   "source": [
    "Precisamos escolher características (colunas) que possam ajudar o sistema a aprender o preço de cada venda. Vamos começar escolhendo variáveis que tenham uma boa correlação com `SalesClosePrice`. Correlação nem sempre implica em características relevantes, mas é um bom lugar pra começar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62391941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03816441",
   "metadata": {},
   "source": [
    "Mas apenas a correlação não me convence. Quero analisar também estas colunas tentando traçar modelos lineares entre tais variáveis e `SalesClosePrice`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9496b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ebd8dc4",
   "metadata": {},
   "source": [
    "Analisando os gráficos, estou convencido que algumas dessas colunas são realmente importantes. Vou escolhê-las e filtrar `real_estate_df` de modo que mantenha apenas elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0668f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81223041",
   "metadata": {},
   "source": [
    "Há duplicatas? E valores nulos? Se sim, vamos removê-los. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca06053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79cb3ea6",
   "metadata": {},
   "source": [
    "Ótimo, não tinhamos nenhum problema com dados faltantes. O próximo passo é normalizarmos os dados. Eles estão com magnitudes diferentes, o que pode interferir no aprendizado do modelo. Podemos utilizar a normalização em z-score para aproximá-los a uma distribuição normal. Vamos fazer o exemplo com `LISTPRICE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4659055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91f414af",
   "metadata": {},
   "source": [
    "Uma vez que todas as colunas estão normalizadas, o dataset está pronto para ser consumido por algum algoritmo de aprendizado de máquina! Bom trabalho!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
