{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a278e3",
   "metadata": {},
   "source": [
    "# Dataframes e SparkSQL\n",
    "\n",
    "Spark oferece abstrações de mais alto nível chamadas APIs estruturadas. Ao todo, são três: Dataframes, Datasets e SparkSQL. \n",
    "\n",
    "![Pilha de APIs, disponível em: https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/](https://izhangzhihao.github.io/assets/images/spark-05.png)\n",
    "\n",
    "Datasets não são implementados em PySpark, porém veremos nesse notebook exemplos de uso das outras duas.\n",
    "\n",
    "**Dataframes** são estruturas tabulares, assim como os DataFrames de Pandas. A diferença, aqui, é que Dataframes em Spark são _distribuídos_ e construídos em cima de RDDs. \n",
    "\n",
    "**SparkSQL** é um conjunto de funcionalidades que são operadas em Dataframes. Veremos que podemos manipular Dataframes tanto programaticamente (através de _transformações_) quanto por linguagem SQL. Além disso, SparkSQL oferece uma série de outras ferramentas para a realização de operações tabulares distribuídas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518f377",
   "metadata": {},
   "source": [
    "Vamos começar importando PySpark e criando um objeto `SparkContext`, assim como fizemos anteriormente. Lembre-se que `SparkContext` é a porta de entrada para manipulação de RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName='Dataframes e SparkSQL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b490bc8",
   "metadata": {},
   "source": [
    "# O objeto SparkSession\n",
    "\n",
    "Para manipularmos Dataframes e utilizar as funções do SparkSQL, porém, precisamos criar um objeto `SparkSession`. O `SparkSession` é um objeto que é construído em cima do SparkContext, e é o ponto de entrada para as APIs estruturadas do Spark. \n",
    "https://imgs.developpaper.com/imgs/147289973-592685e12263f_articlex.png ![SparkSession vs SparkContext em: https://developpaper.com/application-sparksession-sparkcontext-and-rdd-in-spark-and-their-extensions/](https://imgs.developpaper.com/imgs/147289973-592685e12263f_articlex.png)\n",
    "\n",
    "Para inicializar um SparkSession, importamos sua definição de `pyspark.sql` e criamos um objeto com `SparkSession.builder.getOrCreate()`. Esse método checa se já existe um `SparkSession` ativo, e se não existir, cria um novo. Observe que não precisei mencionar o `SparkContext`. Ele recupera sua instância automaticamente, ou a cria se não existir também."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9dc6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83750b7c",
   "metadata": {},
   "source": [
    "# Criando Dataframes\n",
    "\n",
    "Dataframes são objetos tabulares (lembre-se do _Pandas_) distribuídos. Você pode criar um dataframe lendo arquivos csv, JSON, [ORC](https://orc.apache.org/) e [Parquet](https://github.com/apache/parquet-format). Além disso, você pode criar Dataframes a partir de RDDs também. \n",
    "\n",
    "Vamos fazer alguns exemplos com a versão simplificada de leitura e criação de arquivos. Para mais opções, consultar a [documentação](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando de um arquivo csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a partir de um RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e3986",
   "metadata": {},
   "source": [
    "# Schemas\n",
    "\n",
    "Quando lendo de um csv ou criando um Dataframe a partir de um RDD, o Spark pode fazer múltiplas leituras para automaticamente tentar inferir qual é o _Schema_ do Dataframe, ou seja, qual é a tipagem de cada coluna e se ela pode ter ou não valores _null_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checar schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd40fa",
   "metadata": {},
   "source": [
    "A inferência de _Schema_ pode causar lentidão na leitura dos dados, devido a necessidade de eles serem lidos múltiplas vezes. Você pode, alternativamente, definir um _Schema_ e utilizá-lo na leitura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3188edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar um Schema e reimportar flights.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9c087",
   "metadata": {},
   "source": [
    "# Operações em Dataframes\n",
    "\n",
    "Vamos começar a realizar operações em Dataframes. Nesse primeiro momento, iremos ver programaticamente algumas das principais funções para manipulação de Dataframes. A maioria aqui é _transformação_, exceto aquelas que precisam devolver algum retorno para o Programa _Driver_, como `count()`, `show()` e `collect()`.\n",
    "\n",
    "### Visualizar informações sobre um Dataframe\n",
    "\n",
    "Vamos ver algumas funções básicas para retornar informações a respeito do Dataframe como um todo. São elas:\n",
    "- `count()`: retorna o número de linhas\n",
    "- `len(df.columns)`: `df.columns` retorna uma lista com o nome das colunas. Se aplicarmos `len()`, podemos ver a quantidade de colunas em um Dataframe.\n",
    "- `describe()`: retorna estatísticas sobre as colunas de um Dataframe. Podemos definir quais colunas queremos, ou a tabela inteira se não passarmos parâmetros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09769f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e49273c",
   "metadata": {},
   "source": [
    "### Select\n",
    "\n",
    "O comando `select()` opera em nível de **coluna**, filtrando quais colunas você quer exibir. Ele pode ainda criar novas colunas, com métodos que analisam valores de uma ou mais colunas linha a linha. É o equivalente ao comando `SELECT` de uma consulta SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Selecionando id da companhia, origem e destino')\n",
    "print('Selecionando id da companhia, origem e destino e colocando o tempo total de atraso do vôo')\n",
    "print('Selecionando data, origem, destino e o tempo de voo em horas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb1a39",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "A função `filter()` opera em nível de **linhas**, e utiliza operações lógicas que retornam resultados _binários_. É similar ao comando `WHERE` de uma consulta SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Apresentar apenas os vôos com saída adiantada')\n",
    "print('Apresentar todos os vôos que não saíram nem chegaram em JFK')\n",
    "print('Selecionar as colunas dataVoo, origem, destino e exibir apenas os vôos com menos de 4 horas de duração')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fab1c",
   "metadata": {},
   "source": [
    "### GroupBy\n",
    "\n",
    "A função `groupBy()` permite realizar operações de agregação no conjunto de dados. Ele equivale ao comando `GROUP BY` de consultas SQL. Pode receber como parâmetro colunas (onde opera a agregação). Se não receber nenhum parâmetro, realiza a agregação na _tabela inteira_. `groupBy()` retorna um objeto `GroupedData`, que contém várias funções de agregação como `sum()`, `mean()`, `max()`, `min()`, entre outros. A função `agg()` permite que seja feita múltiplas agregações (por exemplo, média e desvio padrão) nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Ver o máximo de cada coluna numérica')\n",
    "print('Ver o tempo médio de atraso de chegada para cada companhia')\n",
    "print('Ver o tempo médio e desvio padrão de duracao para cada vôo específico')\n",
    "print('Ver apenas o vôo com menor atraso total (partida + chegada) de cada par origem-destino')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c45dfc",
   "metadata": {},
   "source": [
    "## Adicionar e remover colunas/linhas\n",
    "\n",
    "Mais algumas funções que manipulam linhas e colunas, porém não criam _subconjuntos_ dos dados como as funções acima.\n",
    "\n",
    "- `withColumn()`: adiciona uma coluna nova ao Dataframe e o retorna\n",
    "- `drop()`: remove uma coluna do Dataframe e o retorna\n",
    "- `distinct()`: remove linhas duplicadas  \n",
    "- `dropDuplicates()`: remove linhas duplicadas, porém é possível passar um subconjunto de colunas para realizar a checagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adicionando uma nova coluna com a duração da da viagem em horas')\n",
    "print('Removendo data, hora_partida e hora_chegada e duracao')\n",
    "print('Removendo duplicatas')\n",
    "print('Mantendo apenas um vôo por origem-destino')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2c88d",
   "metadata": {},
   "source": [
    "### OrderBy\n",
    "\n",
    "A função `orderBy()` aceita dois argumentos: uma coluna (ou lista de colunas) que deverão ser usadas na ordenação, e um _boolean_ (ou lista de _booleans_) definindo ordem ascendente ou não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27977434",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ordenando decrescentemente por duração\")\n",
    "print(\"Ordenando ascendentemente por origem e destino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55862d13",
   "metadata": {},
   "source": [
    "### Join\n",
    "\n",
    "A função `join()` une dois Dataframes a partir de uma coluna em comum. Há vários tipos diferentes de joins no Spark:\n",
    "\n",
    "![Tipos de join. Disponível em: https://medium.com/bild-journal/pyspark-joins-explained-9c4fba124839](https://miro.medium.com/max/622/1*6d7MzkjxS0eBWjOJN5TaAQ.jpeg)\n",
    "\n",
    "Todos os tipos de join mostrados acima podem ser usados junto com o método. Vamos ver dois exemplos: `'left_outer'` e `'inner'`. Os demais tipos estão listados na [documentação](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.join.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90788ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Lendo novos datasets')\n",
    "print('Fazendo left outer join')\n",
    "print('Fazendo inner join')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd06322",
   "metadata": {},
   "source": [
    "# Usando SQL\n",
    "\n",
    "Praticamente todas as operações que vimos acima podem ser descritas diretamente em SQL e aplicadas em um Dataframe. Spark possui um método `sql()` que recebe uma string com sintaxe SQL e a processa de maneira distribuída. O resultado é idêntico ao que fizemos programaticamente. No fim, vai de gosto e familiaridade do programador para escolher qual opção prefere.\n",
    "\n",
    "Antes de realizarmos consultas SQL, é importante que criemos uma visualização temporária SQL com `createOrReplaceTempView()`. Vamos fazer alguns exemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2aac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Distancia media de todos voos')\n",
    "print('Tempo medio de cada itinerario')\n",
    "print('Apenas itinerários com tempo medio  menor que 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c884b1",
   "metadata": {},
   "source": [
    "# Usando funções com pyspark.sql.functions\n",
    "\n",
    "O pacote `pyspark.sql.functions` fornece uma série de implementações de funções padrões para a manipulação de Dataframes, Datasets e consultas SQL. Podemos utilizar neste pacote, dentre muitas outras, funções aritméticas e de transformação de `Strings` e `Arrays`. Vamos importar o pacote e usá-lo da seguinte forma:\n",
    "\n",
    "`import pyspark.sql.functions as F`\n",
    "\n",
    "\n",
    "### Transformações de String\n",
    "\n",
    "Vamos começar com algumas funções de manipulação de String:\n",
    "\n",
    "- `upper()` e `lower()`: transformam o texto inteiro em caixa alta ou baixa, respectivamente.\n",
    "\n",
    "- `split()`: quebram um String, transformando ele em uma lista dentro de uma célula de um Dataframe (coluna se torna do tipo `ArrayType()`.\n",
    "\n",
    "- `contains()`: checa se uma célula contém uma substring\n",
    "\n",
    "- `cast()`: converte um String em outro tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Caixa alta e caixa baixa')\n",
    "print('Quebrando texto em tokens')\n",
    "print('Filtrando por KING')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb238b",
   "metadata": {},
   "source": [
    "### Funções com ArrayType()\n",
    "\n",
    "Dataframes conseguem guardar `Arrays` em suas células, mas não conseguem operar diretamente com elas. A manipulação deve ser feita através de funções padrões em `pyspark.sql.functions` ou de UDFs criadas por usuários (veremos elas logo).\n",
    "\n",
    "- `size()`: retorna o tamanho do Array em uma célula.\n",
    "\n",
    "- `getItem()`: retorna um elemento de um Array em uma célula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Descobrindo o numero de elementos em cada linha:')\n",
    "print('Recuperando o primeiro token de cada linha')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a972b1",
   "metadata": {},
   "source": [
    "### Condicionais\n",
    "\n",
    "O pacote `pyspark.sql.functions` fornece também as funções condicionais `when()` e `otherwise()`. \n",
    "\n",
    "- `when()`: Age como uma cláusula `if`.\n",
    "- `otherwise()`: Corresponde ao `else`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Retornar colunas Split, Primeiro_token e criar uma nova verificando se há a palavra \"king\"')\n",
    "print('Criar coluna dizendo se há \"Claudius\" (valor 1), \"Polonius\" (valor 2) ou outra pessoa (valor 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94c566",
   "metadata": {},
   "source": [
    "### Funções definidas por usuários\n",
    "\n",
    "O pacote `pyspark.sql.functions` fornece também a possibilidade de criar funções que ele não tem implementado por padrão. Essas são chamadas de UDFs, ou _User-Defined Functions_, e têm o seguinte formato:\n",
    "\n",
    "`F.udf(funcão, tipo_de_retorno)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19160890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Contar numero de menções \"hamlet\" na linha')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dfaef",
   "metadata": {},
   "source": [
    "# Exercício: modificando `key_text_df`\n",
    "\n",
    "Vamos transformar o nosso Dataframe usado nos exemplos acima. Faça os seguintes passos:\n",
    "- Remova as linhas onde o campo chave esteja em branco \n",
    "- Crie uma UDF para remover tudo que houver antes do @ na coluna chave e salve em uma coluna chamada 'id_linha'\n",
    "- Crie uma coluna com o texto em caixa baixa e a nomeie de 'texto'\n",
    "- Remova as colunas de chave e valor antigas\n",
    "- Crie uma nova coluna com os tokens de 'texto'. A nomeie de 'tokens'\n",
    "- Crie uma coluna com a quantidade de tokens. A nomeie de 'num_tokens'\n",
    "- Remova as linhas em branco e que contém apenas '|'\n",
    "- Extraia o primeiro e o ultimo token de cada linha, e os guarde em colunas nomeadas por 'primeiro' e 'ultimo'\n",
    "- Filtre o dataframe para manter somente linhas que não mencionam 'hamlet' (utilize o sinal ~ como negação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca996cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "951d4406",
   "metadata": {},
   "source": [
    "# Visualizando Dados graficamente\n",
    "\n",
    "Infelizmente, Spark não suporta visualização de dados nativamente. Para visualizá-los, podemos:\n",
    "- Utilizar bibliotecas de terceiros, como a `pyspark_dist_explore` (não recomendado, última versão em 2019), ou\n",
    "- Retirar uma amostra dos dados e transformá-la em Pandas Dataframes, e aproveitar as bibliotecas `Matplotlib` ou `Seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60221fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00677b93",
   "metadata": {},
   "source": [
    "# Particionamento e otimização\n",
    "\n",
    "Veremos algumas funções aqui que lidam diretamente com configurações relacionadas com o formato distribuído dos Dataframes em Spark.\n",
    "\n",
    "\n",
    "### Criando IDs\n",
    "\n",
    "Em um ambiente distribuído, criar IDs seguindo abordagens como contadores e geradores aleatórios não são ideais, dado que múltiplas partições irão lidar com múltiplos _pedaços_ dos dados. Utilizando a função `monotonically_increasing_id()` do pacote `pyspark.sql.functions` possibilita ao Spark criar IDs de forma distribuída e eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84485d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6afb1c0d",
   "metadata": {},
   "source": [
    "### Manipulando partições\n",
    "\n",
    "Muitas das vezes precisamos manipular em quantas partições dividimos nossos dados, para aumentar eficiência. Para manipular partições usamos as seguintes funções:\n",
    "\n",
    "- `rdd.getNumPartitions()`: recupera a quantidade de partições seu RDD ou Dataframe está alocando.\n",
    "\n",
    "- `repartition()`: modifica a quantidade de partições que seu RDD ou Dataframe está alocando.\n",
    "\n",
    "- `coalesce()`: reduz a quantidade de partições (mais eficiente que `repartition()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379de9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "991a78f9",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "_Caching_ é a operação de guardar Dataframes na memória ou em disco. Essa operação pode ajudar a melhorar o desempenho em transformações tardias e reduzir a quantidade de recursos gastos. **Cuidado:** bases de dados muito grandes podem não caber na memória do cluster, e _swap_ com o disco pode ser custoso.\n",
    "\n",
    "Para guardar em cache seu Dataframe, basta invocar a função `cache()`. Para tirá-lo do cache, utilize a função `unpersist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfc421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4d953ed",
   "metadata": {},
   "source": [
    "# Exemplo prático: pipeline de dados\n",
    "\n",
    "Vamos fazer uma análise um pouco mais detalhada de um dataset. Vamos explorar o _\"2017 St Paul MN Real Estate Dataset\"_ e iremos tentar limpar e transformar ele para que seja utilizável por algum algoritmo de aprendizado de máquina para prever o **preço de venda** de um imóvel. Começaremos importando ele e checando seu Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5f999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e42ed3",
   "metadata": {},
   "source": [
    "Olhando o Schema, conseguimos ver que a variável que gostaríamos de prever é a `SalesClosePrice`. Vamos ver sua distribuição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23fd89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9268fbe5",
   "metadata": {},
   "source": [
    "Precisamos escolher características (colunas) que possam ajudar o sistema a aprender o preço de cada venda. Vamos começar escolhendo variáveis que tenham uma boa correlação com `SalesClosePrice`. Correlação nem sempre implica em características relevantes, mas é um bom lugar pra começar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62391941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03816441",
   "metadata": {},
   "source": [
    "Mas apenas a correlação não me convence. Quero analisar também estas colunas tentando traçar modelos lineares entre tais variáveis e `SalesClosePrice`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9496b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ebd8dc4",
   "metadata": {},
   "source": [
    "Analisando os gráficos, estou convencido que algumas dessas colunas são realmente importantes. Vou escolhê-las e filtrar `real_estate_df` de modo que mantenha apenas elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0668f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81223041",
   "metadata": {},
   "source": [
    "Há duplicatas? E valores nulos? Se sim, vamos removê-los. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca06053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79cb3ea6",
   "metadata": {},
   "source": [
    "Ótimo, não tinhamos nenhum problema com dados faltantes. O próximo passo é normalizarmos os dados. Eles estão com magnitudes diferentes, o que pode interferir no aprendizado do modelo. Podemos utilizar a normalização em z-score para aproximá-los a uma distribuição normal. Vamos fazer o exemplo com `LISTPRICE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4659055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91f414af",
   "metadata": {},
   "source": [
    "Uma vez que todas as colunas estão normalizadas, o dataset está pronto para ser consumido por algum algoritmo de aprendizado de máquina! Bom trabalho!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
